{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd9e31d3-54e1-4a0d-984c-ec0f5d0debb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68090de5-6638-4445-a62e-9b4a76120c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_id='M05'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ea0c6f4-fecb-4863-83ae-8a5a633f666d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Whisper(\n",
       "  (encoder): AudioEncoder(\n",
       "    (conv1): Conv1d(80, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (blocks): ModuleList(\n",
       "      (0-31): 32 x ResidualAttentionBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): TextDecoder(\n",
       "    (token_embedding): Embedding(51865, 1280)\n",
       "    (blocks): ModuleList(\n",
       "      (0-31): 32 x ResidualAttentionBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (cross_attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (cross_attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from dataclasses import dataclass, field, replace\n",
    "from typing import TYPE_CHECKING, Dict, Iterable, List, Optional, Sequence, Tuple, Union\n",
    "import whisper_openAI.whisper as whisper\n",
    "import torch\n",
    "from whisper_openAI.whisper.tokenizer import Tokenizer, get_tokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "# We get the acoustic embeddings from Whisper Large V2\n",
    "model,processor = whisper.load_model(\"large-v2\")\n",
    "# model,processor = whisper.load_model(\"medium\")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbfdfa88-7879-479a-b474-2f093860401f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "# The below is the json file you can generate using the \"To generatn-best hyporhesis.ipynb\" notebook; Need to further tokenize the hypothesis\n",
    "\n",
    "with open(f'Inference/gs_inferences/torgo_train_{speaker_id}.json', \"r\") as file:  # Change the file path and name here\n",
    "    train_data = json.load(file)\n",
    "\n",
    "with open(f'Inference/gs_inferences/torgo_val_{speaker_id}.json', \"r\") as valid_file:\n",
    "    val_data = json.load(valid_file)\n",
    "\n",
    "# Load the test set\n",
    "with open(f'Inference/gs_inferences/torgo_test_{speaker_id}.json', \"r\") as test_file:\n",
    "    test_data = json.load(test_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c0960fd-df96-42c5-875d-133e35093e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train has 9,502 samples\n",
      "Processing train split ...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Implementation derived from https://github.com/tloen/alpaca-lora\"\"\"\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import requests\n",
    "import json\n",
    "import os \n",
    "\n",
    "from lit_llama.tokenizer import Tokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "tokenizer_path: Path = Path(\"weights/tokenizer.model\")\n",
    "tokenizer = Tokenizer(tokenizer_path)\n",
    "print(f\"train has {len(train_data):,} samples\")\n",
    "print(\"Processing train split ...\")\n",
    "\n",
    "\n",
    "\n",
    "def tokenize(tokenizer: Tokenizer, string: str, max_length: int, eos=True) -> torch.Tensor:\n",
    "    return tokenizer.encode(string, bos=True, eos=eos, max_length=max_length)\n",
    "    \n",
    "def process_train_data(train_data):\n",
    "    instruction = 'You are an ASR transcript selector. You have a few transcripts generated by an automatic speech recognition model. Your task is to generate the most likely transcript from them. If the generated transcripts have grammatical or logical errors, you will modify them accordingly to produce the most accurate and coherent transcript.'\n",
    "    result = []\n",
    "\n",
    "    for i in tqdm(range(len(train_data))):        \n",
    "        for name in train_data[i].keys():\n",
    "            ip = train_data[i][name]\n",
    "        inference = ip['inference']\n",
    "        gt = ip['ground_truth']\n",
    "            \n",
    "        # Removing the ground_truth, if present among the inferences for the prompt\n",
    "        if gt in inference:\n",
    "            inference.remove(gt)\n",
    "                \n",
    "        # Joining the inputs with '\\n'\n",
    "        for_input = '\\n'.join(inference[:15])\n",
    "        # The prompt follows the Alpaca template\n",
    "        full_prompt = f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{for_input}\\n\\n### Response:\"\"\"\n",
    "        full_prompt_and_response = full_prompt + gt\n",
    "\n",
    "        encoded_full_prompt = tokenize(tokenizer, full_prompt, max_length=2048, eos=False)\n",
    "        encoded_full_prompt_and_response = tokenize(tokenizer, full_prompt_and_response, eos=True, max_length=2048)\n",
    "        labels = encoded_full_prompt_and_response.clone()\n",
    "        labels_with_masked_input = encoded_full_prompt_and_response.clone()\n",
    "        labels_with_masked_input[:len(encoded_full_prompt)] = -1\n",
    "        \n",
    "        path = ip['path']\n",
    "        audio = whisper.load_audio(path)  \n",
    "        audio = whisper.pad_or_trim(audio)            \n",
    "        mel = whisper.log_mel_spectrogram(audio).to(model.device)  # Adjust as needed for your model\n",
    "        mel = mel.unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            audio_features = model.encoder(mel)\n",
    "        \n",
    "        result.append({**ip, 'index': name, \"input_ids\": encoded_full_prompt_and_response, \"input_ids_no_response\": encoded_full_prompt, \"labels\": labels, 'labels_with_masked_input': labels_with_masked_input, 'audio_features': audio_features.bfloat16()})\n",
    "\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f65a2438-e80a-4139-94e4-da9c91a8ece9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 43/9502 [00:14<53:30,  2.95it/s] \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "process_train_data(train_data)\n",
    "torch.save(result,f'Inference/gs_inferences/torgo_{speaker_id}_train.pt')\n",
    "print(f\"Processed training data and saved checkpoint\")\n",
    "\n",
    "split = \"val\"\n",
    "result = process_train_data(validation_dataset)\n",
    "torch.save(result,f'Inference/gs_inferences/torgo_{speaker_id}_{split}.pt')\n",
    "print(f\"Processed training data and saved checkpoint\")\n",
    "\n",
    "split = \"val\"\n",
    "result = process_train_data(test_data)\n",
    "torch.save(result,f'Inference/gs_inferences/torgo_{speaker_id}_{split}.pt')\n",
    "print(f\"Processed training data and saved checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bb225c2-30b0-40b8-8071-d508a5eff12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.26.0\n",
      "  Downloading numpy-1.26.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m250.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.0\n",
      "    Uninstalling numpy-2.0.0:\n",
      "      Successfully uninstalled numpy-2.0.0\n",
      "\u001b[33m  WARNING: Failed to remove contents in a temporary directory '/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/~-mpy.libs'.\n",
      "  You can safely remove it manually.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: Failed to remove contents in a temporary directory '/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/~~mpy'.\n",
      "  You can safely remove it manually.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "numba 0.59.1 requires llvmlite<0.43,>=0.42.0dev0, but you have llvmlite 0.43.0 which is incompatible.\n",
      "streamlit 1.32.0 requires packaging<24,>=16.8, but you have packaging 24.1 which is incompatible.\n",
      "streamlit 1.32.0 requires protobuf<5,>=3.20, but you have protobuf 5.27.1 which is incompatible.\n",
      "tensorboard 2.15.1 requires protobuf<4.24,>=3.19.6, but you have protobuf 5.27.1 which is incompatible.\n",
      "torchvision 0.15.2+cu118 requires torch==2.0.1, but you have torch 2.3.1 which is incompatible.\n",
      "xformers 0.0.22.post7 requires torch==2.1.0, but you have torch 2.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.0\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.26.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "684fe581-49a5-4039-8963-a86dea730fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numba in /work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages (0.59.1)\n",
      "Collecting numba\n",
      "  Downloading numba-0.60.0-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages (from numba) (0.43.0)\n",
      "Requirement already satisfied: numpy<2.1,>=1.22 in /work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages (from numba) (1.26.0)\n",
      "Downloading numba-0.60.0-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numba\n",
      "  Attempting uninstall: numba\n",
      "    Found existing installation: numba 0.59.1\n",
      "    Uninstalling numba-0.59.1:\n",
      "      Successfully uninstalled numba-0.59.1\n",
      "Successfully installed numba-0.60.0\n"
     ]
    }
   ],
   "source": [
    "!pip install numba --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e15b77-247c-46cd-b908-7bee8d386738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511a132b-2a92-4e1f-a5fb-170fa5a3d832",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
