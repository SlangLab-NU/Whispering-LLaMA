{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a158536-6e39-4582-9a14-c47c6b82df18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import requests\n",
    "import json\n",
    "import os \n",
    "\n",
    "model_name=\"large-v2\"\n",
    "pattern = \"n_best_phoneme\"\n",
    "\n",
    "\n",
    "\n",
    "from dataclasses import dataclass, field, replace\n",
    "from typing import TYPE_CHECKING, Dict, Iterable, List, Optional, Sequence, Tuple, Union\n",
    "import whisper_openAI.whisper as whisper\n",
    "import torch\n",
    "from whisper_openAI.whisper.tokenizer import Tokenizer, get_tokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import json\n",
    "\n",
    "# We get the acoustic embeddings from Whisper Large V2\n",
    "model,processor = whisper.load_model(\"large-v2\")\n",
    "# model,processor = whisper.load_model(\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12db7520-ae60-464d-aaca-9d2f957d713f",
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_id = 'M03'\n",
    "test_speaker = speaker_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717cea24-a650-4566-be6b-789b53b559bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train has 9,376 samples\n"
     ]
    }
   ],
   "source": [
    "with open(f'Inference/gs_inferences/large-v2_hypo/torgo_train_{speaker_id}_{model_name}.json', \"r\") as file:  # Change the file path and name here\n",
    "    train_data = json.load(file)\n",
    "\n",
    "with open(f'Inference/gs_inferences/large-v2_hypo/torgo_val_{speaker_id}_{model_name}.json', \"r\") as valid_file:\n",
    "    val_data = json.load(valid_file)\n",
    "\n",
    "# Load the test set\n",
    "with open(f'Inference/gs_inferences/large-v2_hypo/torgo_test_{speaker_id}_{model_name}.json', \"r\") as test_file:\n",
    "    test_data = json.load(test_file)\n",
    "    \n",
    "\n",
    "from lit_llama.tokenizer import Tokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "tokenizer_path: Path = Path(\"weights/tokenizer.model\")\n",
    "tokenizer = Tokenizer(tokenizer_path)\n",
    "print(f\"train has {len(train_data):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dec66233-7d1b-47b4-ba65-a64338d11d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of old_data_train: 9376\n",
      "Length of old_data_val: 460\n",
      "Length of old_data_test: 442\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "old_folder = \"tiny_hypo\"\n",
    "old_data_train = torch.load(f'Inference/gs_inferences/{old_folder}/torgo_{test_speaker}_train.pt',map_location=torch.device('cpu'))\n",
    "old_data_val = torch.load(f'Inference/gs_inferences/{old_folder}/torgo_{test_speaker}_val.pt',map_location=torch.device('cpu'))\n",
    "old_data_test = torch.load(f'Inference/gs_inferences/{old_folder}/torgo_{test_speaker}_test.pt',map_location=torch.device('cpu'))\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "# Print the lengths of the loaded data\n",
    "print(f'Length of old_data_train: {len(old_data_train)}')\n",
    "print(f'Length of old_data_val: {len(old_data_val)}')\n",
    "print(f'Length of old_data_test: {len(old_data_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92e48a18-36b6-48ca-938c-5a2309473c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'F01-Session1-arrayMic-0009': {'temp': 0.74,\n",
       "  'path': '/work/van-speech-nlp/data/torgo/F01/Session1/wav_arrayMic/0009.wav',\n",
       "  'ground_truth': 'pat',\n",
       "  'inference': ['',\n",
       "   'ah',\n",
       "   'ah!',\n",
       "   'alright',\n",
       "   'bartender 3',\n",
       "   'berk',\n",
       "   'bleep',\n",
       "   'blurp',\n",
       "   'burp',\n",
       "   'burp!',\n",
       "   'burps',\n",
       "   'but',\n",
       "   'bye!',\n",
       "   'ehh',\n",
       "   'ha',\n",
       "   'hah',\n",
       "   'hehe',\n",
       "   'hot!',\n",
       "   'hrrrgh',\n",
       "   'huh',\n",
       "   'hurt!',\n",
       "   'huuuuh!',\n",
       "   'im gonna go',\n",
       "   'oh',\n",
       "   'purr!',\n",
       "   'rip camera man',\n",
       "   'thanks for watching see you next time!',\n",
       "   'ugh',\n",
       "   'uh',\n",
       "   'uhoh',\n",
       "   'urgh!'],\n",
       "  'source': 'NP-Torgo',\n",
       "  'category': 'NP-Torgo',\n",
       "  'time': 30.0}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb88066c-fb13-4eda-b757-1b62b6f5064b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from g2p_en import G2p\n",
    "import re\n",
    "\n",
    "# Define the convert_to_phonemes function\n",
    "def convert_to_phonemes(sentences, remove_num=True):\n",
    "    g2p = G2p()\n",
    "    phoneme_sequences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        phonemes = \" \".join(g2p(sentence))\n",
    "        if remove_num:\n",
    "            phonemes = re.sub(r'\\d+', '', phonemes)\n",
    "        phoneme_sequences.append(phonemes)\n",
    "\n",
    "    return phoneme_sequences\n",
    "\n",
    "def tokenize(tokenizer: Tokenizer, string: str, max_length: int, eos=True) -> torch.Tensor:\n",
    "    return tokenizer.encode(string, bos=True, eos=eos, max_length=max_length)\n",
    "    \n",
    "# Define the process_train_data function\n",
    "def process_train_data(train_data, old_data):\n",
    "    instruction = 'You are an ASR transcript selector. You have a few transcripts generated by an automatic speech recognition model, and their corresponding phonemes. Your task is to generate the most likely transcript from them. If the generated transcripts have grammatical or logical errors, you will modify them accordingly to produce the most accurate and coherent transcript.'\n",
    "    result = []\n",
    "\n",
    "    for i in tqdm(range(len(train_data[:2]))):\n",
    "        for name in train_data[i].keys():\n",
    "            ip = train_data[i][name]\n",
    "        \n",
    "        inference = ip['inference']\n",
    "        gt = ip['ground_truth']\n",
    "        path = ip['path']\n",
    "        \n",
    "        # Removing the ground_truth, if present among the inferences for the prompt\n",
    "        if gt in inference:\n",
    "            inference.remove(gt)\n",
    "\n",
    "        print(gt)\n",
    "        print(inference)  # Print the number of inferences for each prompt\n",
    "        \n",
    "        # Convert inference to phoneme sequences\n",
    "        phoneme_sequences = convert_to_phonemes(inference[:15])\n",
    "        print(phoneme_sequences)\n",
    "                                                                                                                          b6\n",
    "        # Joining the inputs with '\\n'\n",
    "        for_input = \"Possible Transcripts: \\n\" + '\\n'.join(inference[:15])\n",
    "        for_input += \"\\n\" + \"Possible Phoenme Sequences: \\n\" + '\\n'.join(phoneme_sequences[:15])\n",
    "          \n",
    "        print(f\"{for_input} \\n\")\n",
    "        # The prompt follows the Alpaca template\n",
    "        full_prompt = f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{for_input}\\n\\n### Response:\"\"\"\n",
    "        full_prompt_and_response = full_prompt + gt\n",
    "\n",
    "        encoded_full_prompt = tokenize(tokenizer, full_prompt, max_length=2048, eos=False)\n",
    "        encoded_full_prompt_and_response = tokenize(tokenizer, full_prompt_and_response, eos=True, max_length=2048)\n",
    "        labels = encoded_full_prompt_and_response.clone()\n",
    "        labels_with_masked_input = encoded_full_prompt_and_response.clone()\n",
    "        labels_with_masked_input[:len(encoded_full_prompt)] = -1\n",
    "        \n",
    "        audio_features = old_data[i][\"audio_features\"]\n",
    "        \n",
    "        result.append({**ip, 'index': name, \"input_ids\": encoded_full_prompt_and_response, \"input_ids_no_response\": encoded_full_prompt, \"labels\": labels, 'labels_with_masked_input': labels_with_masked_input, 'audio_features': audio_features.bfloat16()})\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aeffe1a1-08e3-4416-86ed-ed9d1151fa3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pat\n",
      "['', 'ah', 'ah!', 'alright', 'bartender 3', 'berk', 'bleep', 'blurp', 'burp', 'burp!', 'burps', 'but', 'bye!', 'ehh', 'ha', 'hah', 'hehe', 'hot!', 'hrrrgh', 'huh', 'hurt!', 'huuuuh!', 'im gonna go', 'oh', 'purr!', 'rip camera man', 'thanks for watching see you next time!', 'ugh', 'uh', 'uhoh', 'urgh!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:01<00:01,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'AA', 'AA   !', 'AO L R AY T', 'B AA R T EH N D ER   TH R IY', 'B ER K', 'B L IY P', 'B L ER P', 'B ER P', 'B ER P   !', 'B ER P S', 'B AH T', 'B AY   !', 'EH HH', 'HH AA']\n",
      "Possible Transcripts: \n",
      "\n",
      "ah\n",
      "ah!\n",
      "alright\n",
      "bartender 3\n",
      "berk\n",
      "bleep\n",
      "blurp\n",
      "burp\n",
      "burp!\n",
      "burps\n",
      "but\n",
      "bye!\n",
      "ehh\n",
      "ha\n",
      "Possible Phoenme Sequences: \n",
      "\n",
      "AA\n",
      "AA   !\n",
      "AO L R AY T\n",
      "B AA R T EH N D ER   TH R IY\n",
      "B ER K\n",
      "B L IY P\n",
      "B L ER P\n",
      "B ER P\n",
      "B ER P   !\n",
      "B ER P S\n",
      "B AH T\n",
      "B AY   !\n",
      "EH HH\n",
      "HH AA \n",
      "\n",
      "meat\n",
      "['eat', 'great', 'heh heeh', 'hey', 'hey hey', 'lets not get ahead of ourselves', 'little asleep', 'neat', 'right', 'wait', 'wait wait', 'we', 'ねえ。 nee']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IY T', 'G R EY T', 'HH EH   HH IY HH', 'HH EY', 'HH EY   HH EY', 'L EH T S   N AA T   G EH T   AH HH EH D   AH V   AW ER S EH L V Z', 'L IH T AH L   AH S L IY P', 'N IY T', 'R AY T', 'W EY T', 'W EY T   W EY T', 'W IY', 'N IY']\n",
      "Possible Transcripts: \n",
      "eat\n",
      "great\n",
      "heh heeh\n",
      "hey\n",
      "hey hey\n",
      "lets not get ahead of ourselves\n",
      "little asleep\n",
      "neat\n",
      "right\n",
      "wait\n",
      "wait wait\n",
      "we\n",
      "ねえ。 nee\n",
      "Possible Phoenme Sequences: \n",
      "IY T\n",
      "G R EY T\n",
      "HH EH   HH IY HH\n",
      "HH EY\n",
      "HH EY   HH EY\n",
      "L EH T S   N AA T   G EH T   AH HH EH D   AH V   AW ER S EH L V Z\n",
      "L IH T AH L   AH S L IY P\n",
      "N IY T\n",
      "R AY T\n",
      "W EY T\n",
      "W EY T   W EY T\n",
      "W IY\n",
      "N IY \n",
      "\n",
      "Processed train data and saved checkpoint for M03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "split = \"train\"\n",
    "result = process_train_data(train_data, old_data_train)\n",
    "torch.save(result,f'Inference/gs_inferences/torgo_{speaker_id}_{model_name}_{pattern}_{split}.pt')\n",
    "print(f\"Processed {split} data and saved checkpoint for {speaker_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963a58de-c0d0-46c1-8e06-06cd9cfec717",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dba8c76-4d4e-4abd-a187-27e3f4e84f9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52306f6-087c-407c-8ef3-9a7a1a9e8975",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a9f4f6-a6ca-4ab0-9df3-273a0499c55b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
