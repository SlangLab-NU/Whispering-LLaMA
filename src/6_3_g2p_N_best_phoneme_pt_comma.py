import os
import sys
# Specify the directory path
new_directory = '/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA'

# Change the current working directory to the specified path
os.chdir(new_directory)

# Verify the change by printing the current working directory
print("Current working directory:", os.getcwd())
sys.path.append(new_directory)


from pathlib import Path
import torch
import requests
import json
import os 
from dataclasses import dataclass, field, replace
from typing import TYPE_CHECKING, Dict, Iterable, List, Optional, Sequence, Tuple, Union
import whisper_openAI.whisper as whisper
import torch
from whisper_openAI.whisper.tokenizer import Tokenizer, get_tokenizer
import torch
import torch.nn.functional as F
from torch import Tensor
import json
import argparse


parser = argparse.ArgumentParser()


# We get the acoustic embeddings from Whisper Large V2
model,processor = whisper.load_model("large-v2")
# model,processor = whisper.load_model("medium")

parser.add_argument('--speaker_id', type=str, choices=[
    'F01', 'F03', 'F04', 'M01', 'M02', 'M03', 'M04', 'M05'
], required=True, help='ID of the speaker to process (choose from the predefined list)')  


model_name="large-v2"
model.eval()


args = parser.parse_args()
speaker_id = args.speaker_id
test_speaker=speaker_id
print(f"Processing speaker {speaker_id}")

test_speaker = speaker_id
pattern = "n_best_phoneme_comma"


with open(f'Inference/gs_inferences/large-v2_hypo/torgo_train_{speaker_id}_{model_name}.json', "r") as file:  # Change the file path and name here
    train_data = json.load(file)

with open(f'Inference/gs_inferences/large-v2_hypo/torgo_val_{speaker_id}_{model_name}.json', "r") as valid_file:
    val_data = json.load(valid_file)

# Load the test set
with open(f'Inference/gs_inferences/large-v2_hypo/torgo_test_{speaker_id}_{model_name}.json', "r") as test_file:
    test_data = json.load(test_file)
    

from lit_llama.tokenizer import Tokenizer
from tqdm import tqdm


tokenizer_path: Path = Path("weights/tokenizer.model")
tokenizer = Tokenizer(tokenizer_path)
print(f"train has {len(train_data):,} samples")


import torch
old_folder = "tiny_hypo"
old_data_train = torch.load(f'Inference/gs_inferences/{old_folder}/torgo_{test_speaker}_train.pt',map_location=torch.device('cpu'))
old_data_val = torch.load(f'Inference/gs_inferences/{old_folder}/torgo_{test_speaker}_val.pt',map_location=torch.device('cpu'))
old_data_test = torch.load(f'Inference/gs_inferences/{old_folder}/torgo_{test_speaker}_test.pt',map_location=torch.device('cpu'))


# In[8]:


# Print the lengths of the loaded data
print(f'Length of old_data_train: {len(old_data_train)}')
print(f'Length of old_data_val: {len(old_data_val)}')
print(f'Length of old_data_test: {len(old_data_test)}')


from g2p_en import G2p
import re

# Define the convert_to_phonemes function
def convert_to_phonemes(sentences, remove_num=False):
    g2p = G2p()
    phoneme_sequences = []

    for sentence in sentences:
        try:
            phonemes = " ".join(g2p(sentence))
            if remove_num:
                phonemes = re.sub(r'\d+', '', phonemes)
            phoneme_sequences.append(phonemes)
        except Exception as e:
            print(f"Error processing sentence: {sentence}")
            print(f"Error: {e}")
            phoneme_sequences.append("")

    return phoneme_sequences

def tokenize(tokenizer: Tokenizer, string: str, max_length: int, eos=True) -> torch.Tensor:
    return tokenizer.encode(string, bos=True, eos=eos, max_length=max_length)
    
# Define the process_train_data function
def process_train_data(train_data, old_data):
    instruction = 'You are an ASR transcript selector. You have a few transcripts and their corresponding phoneme expansions generated by an automatic speech recognition model. Your task is to generate the most likely transcript from them. If the generated transcripts have grammatical or logical errors, you will modify them accordingly to produce the most accurate and coherent transcript.'
    result = []

    for i in tqdm(range(len(train_data))):
        for name in train_data[i].keys():
            ip = train_data[i][name]
        
        inference = ip['inference']
        gt = ip['ground_truth']
        path = ip['path']
        
        # Removing the ground_truth, if present among the inferences for the prompt
        if gt in inference:
            inference.remove(gt)
        inference = [inf for inf in inference if inf]  # Remove empty inferences


        # Convert inference to phoneme sequences
        phoneme_sequences = convert_to_phonemes(inference[:15])
        
        for_input = ""
        for j in range(len(inference[:15])):
            for_input += f"{inference[j]}, {phoneme_sequences[j]}\n"

          
        print(f"{for_input} \n")
        # The prompt follows the Alpaca template
        full_prompt = f"""Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\n### Input:\n{for_input}\n\n### Response:"""
        full_prompt_and_response = full_prompt + gt

        encoded_full_prompt = tokenize(tokenizer, full_prompt, max_length=2048, eos=False)
        encoded_full_prompt_and_response = tokenize(tokenizer, full_prompt_and_response, eos=True, max_length=2048)
        labels = encoded_full_prompt_and_response.clone()
        labels_with_masked_input = encoded_full_prompt_and_response.clone()
        labels_with_masked_input[:len(encoded_full_prompt)] = -1
        
        audio_features = old_data[i]["audio_features"]
        
        result.append({**ip, 'index': name, "input_ids": encoded_full_prompt_and_response, "input_ids_no_response": encoded_full_prompt, "labels": labels, 'labels_with_masked_input': labels_with_masked_input, 'audio_features': audio_features.bfloat16()})
    
    return result




output_path = 'Inference/gs_inferences'
# Construct the full directory path
directory = os.path.join(output_path, pattern)

# Check if the directory exists, if not, create it
if not os.path.exists(directory):
    os.makedirs(directory)
    
split = "train"
result = process_train_data(train_data, old_data_train)
torch.save(result,f'{output_path}/{pattern}/torgo_{speaker_id}_{model_name}_{split}.pt')
print(f"Processed {split} data and saved checkpoint for {speaker_id}")

split = "val"
result = process_train_data(val_data, old_data_val)
torch.save(result,f'{output_path}/{pattern}/torgo_{speaker_id}_{model_name}_{split}.pt')
print(f"Processed {split} data and saved checkpoint for {speaker_id}")

split = "test"
result = process_train_data(test_data, old_data_test)
torch.save(result,f'{output_path}/{pattern}/torgo_{speaker_id}_{model_name}_{split}.pt')
print(f"Processed {split} data and saved checkpoint for {speaker_id}")