{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c69ce4a5-7f60-4f62-b932-646304a61210",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/slam-llm/transformers/src/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/slam-llm/transformers/src/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/slam-llm/transformers/src/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker ID: M03\n",
      "Learning rate: 0.0001\n",
      "Training batch size: 4\n",
      "Evaluation batch size: 4\n",
      "Random seed: 42\n",
      "Gradient accumulation steps: 2\n",
      "Optimizer type: adamw_torch\n",
      "Learning rate scheduler type: linear\n",
      "Number of epochs: 20\n",
      "Keep all data: False\n",
      "Debug mode: False\n",
      "Repository suffix: \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset, DatasetDict, Audio\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union\n",
    "from evaluate import load\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from jiwer import wer\n",
    "\n",
    "\n",
    "# parser = argparse.ArgumentParser(\n",
    "#     description='Process speaker ID and optional parameters.')\n",
    "# # Required argument: speaker ID\n",
    "# parser.add_argument('--speaker_id',\n",
    "#                     type=str,\n",
    "#                     help='Speaker ID in the format [MF]C?[0-9]{2}')\n",
    "# parser.add_argument(\"--model_name\", type=str, help=\"Name of the Whisper model to load (e.g., 'tiny', 'base', 'small', 'medium', 'large')\")\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "speaker_id = 'M03'\n",
    "test_speaker = speaker_id\n",
    "\n",
    "learning_rate = 0.0001\n",
    "train_batch_size = 4\n",
    "eval_batch_size = 4\n",
    "seed = 42\n",
    "gradient_accumulation_steps = 2\n",
    "optimizer = \"adamw_torch\"\n",
    "lr_scheduler_type = \"linear\"\n",
    "num_epochs = 20\n",
    "keep_all_data = False\n",
    "debug = False\n",
    "repo_suffix = \"\"\n",
    "\n",
    "print(f\"Speaker ID: {speaker_id}\")\n",
    "print(f\"Learning rate: {learning_rate}\")\n",
    "print(f\"Training batch size: {train_batch_size}\")\n",
    "print(f\"Evaluation batch size: {eval_batch_size}\")\n",
    "print(f\"Random seed: {seed}\")\n",
    "print(f\"Gradient accumulation steps: {gradient_accumulation_steps}\")\n",
    "print(f\"Optimizer type: {optimizer}\")\n",
    "print(f\"Learning rate scheduler type: {lr_scheduler_type}\")\n",
    "print(f\"Number of epochs: {num_epochs}\")\n",
    "print(f\"Keep all data: {keep_all_data}\")\n",
    "print(f\"Debug mode: {debug}\")\n",
    "print(f\"Repository suffix: {repo_suffix}\")\n",
    "test_speaker = speaker_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea1b3bd6-bcdf-40b7-9aa2-4184cc5c8e09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/work/van-speech-nlp/jindaznb/visenv'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d45907a8-4db3-4140-be8d-49a3d617964e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CSV file exists.\n",
      "torgo_dataset_path: /work/van-speech-nlp/data/torgo\n",
      "torgo_dataset_dir_path: /work/van-speech-nlp/data/torgo/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "# Define the path to the CSV file\n",
    "torgo_csv_path = \"data_preparation/torgo.csv\"\n",
    "\n",
    "# Check if the path exists and is a file\n",
    "if os.path.exists(torgo_csv_path) and os.path.isfile(torgo_csv_path):\n",
    "    print(\"The CSV file exists.\")\n",
    "else:\n",
    "    print(\"The CSV file does not exist.\")\n",
    "\n",
    "torgo_dataset_path = '/work/van-speech-nlp/data/torgo'\n",
    "torgo_dataset_dir_path = torgo_dataset_path + \\\n",
    "        '/' if torgo_dataset_path[-1] != '/' else torgo_dataset_path\n",
    "output_path = 'output'\n",
    "print(f'torgo_dataset_path: {torgo_dataset_path}')\n",
    "print(f'torgo_dataset_dir_path: {torgo_dataset_dir_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83f291c9-eaf0-4eb7-94b2-0e524ba40f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/22/2024 18:22:21 - INFO - huggingsound.speech_recognition.model - Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e403d61774748a8aa81d63fb29b4fe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/slam-llm/transformers/src/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cc674941c344c919103394cf7e12e3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.26G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at jonatasgrosman/wav2vec2-large-xlsr-53-english were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_v', 'wav2vec2.encoder.pos_conv_embed.conv.weight_g']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at jonatasgrosman/wav2vec2-large-xlsr-53-english and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bda6e99da4e42b7989d12ad6fce10ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/262 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84f7ca9e221c43798fc6ed919a0f5d16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/300 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7951ed7aeb504debb807230c49c2c109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "# Renamed the Whisepr repo (https://github.com/openai/whisper) with the changed decoding.py file as whisper_openAI\n",
    "import whisper_openAI.whisper as whisper\n",
    "import torch\n",
    "import tqdm\n",
    "from huggingsound import SpeechRecognitionModel\n",
    "\n",
    "model = SpeechRecognitionModel(\"jonatasgrosman/wav2vec2-large-xlsr-53-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9104a532-a7e8-424c-8f02-d86604df2659",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/google/protobuf/internal/api_implementation.py:89: UserWarning: Selected implementation cpp is not available.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "/work/van-speech-nlp/jindaznb/mmenv/bin/../lib/libstdc++.so.6: version `GLIBCXX_3.4.29' not found (required by /work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/google/protobuf/pyext/_message.cpython-39-x86_64-linux-gnu.so)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/wandb/__init__.py:27\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# This needs to be early as other modules call it.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mterm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m termsetup, termlog, termerror, termwarn\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sdk \u001b[38;5;28;01mas\u001b[39;00m wandb_sdk\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[1;32m     31\u001b[0m wandb\u001b[38;5;241m.\u001b[39mwandb_lib \u001b[38;5;241m=\u001b[39m wandb_sdk\u001b[38;5;241m.\u001b[39mlib  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/wandb/sdk/__init__.py:25\u001b[0m\n\u001b[1;32m      3\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSettings\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhelper\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wandb_helper \u001b[38;5;28;01mas\u001b[39;00m helper\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01martifacts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01martifact\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Artifact\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwandb_alerts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AlertLevel\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwandb_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/wandb/sdk/artifacts/artifact.py:46\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data_types, env, util\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnormalize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m normalize_exceptions\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpublic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArtifactCollection, ArtifactFiles, RetryingClient, Run\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_types\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WBValue\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/wandb/apis/__init__.py:43\u001b[0m\n\u001b[1;32m     38\u001b[0m     _disable_ssl()\n\u001b[1;32m     41\u001b[0m reset_path \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mvendor_setup()\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Api \u001b[38;5;28;01mas\u001b[39;00m InternalApi  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpublic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Api \u001b[38;5;28;01mas\u001b[39;00m PublicApi  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m     46\u001b[0m reset_path()\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/wandb/apis/internal.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Api \u001b[38;5;28;01mas\u001b[39;00m InternalApi\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mApi\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Internal proxy to the official internal API.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/wandb/sdk/internal/internal_api.py:47\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgql_request\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GraphQLSession\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhashutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m B64MD5, md5_file_b64\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m retry\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfilenames\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DIFF_FNAME, METADATA_FNAME\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgitlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GitRepo\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/wandb/sdk/lib/retry.py:17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CheckRetryFnType\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmailbox\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ContextCancelledError\n\u001b[1;32m     19\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# To let tests mock out the retry logic's now()/sleep() funcs, this file\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# should only use these variables, not call the stdlib funcs directly.\u001b[39;00m\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/wandb/sdk/lib/mailbox.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, Callable, Dict, List, Optional, Tuple\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Error\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wandb_internal_pb2 \u001b[38;5;28;01mas\u001b[39;00m pb\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterface_shared\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InterfaceShared\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/wandb/proto/wandb_internal_pb2.py:10\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproto\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv4\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwandb_internal_pb2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protobuf_version \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproto\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv5\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwandb_internal_pb2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/wandb/proto/v5/wandb_internal_pb2.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Generated by the protocol buffer compiler.  DO NOT EDIT!\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# source: wandb/proto/wandb_internal.proto\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Protobuf Python Version: 5.26.1\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"Generated protocol buffer code.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m descriptor \u001b[38;5;28;01mas\u001b[39;00m _descriptor\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m descriptor_pool \u001b[38;5;28;01mas\u001b[39;00m _descriptor_pool\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m symbol_database \u001b[38;5;28;01mas\u001b[39;00m _symbol_database\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/google/protobuf/descriptor.py:28\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[38;5;66;03m# TODO: Remove this import after fix api_implementation\u001b[39;00m\n\u001b[1;32m     27\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m _message \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _message\n\u001b[1;32m     29\u001b[0m   _USE_C_DESCRIPTORS \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mError\u001b[39;00m(\u001b[38;5;167;01mException\u001b[39;00m):\n",
      "\u001b[0;31mImportError\u001b[0m: /work/van-speech-nlp/jindaznb/mmenv/bin/../lib/libstdc++.so.6: version `GLIBCXX_3.4.29' not found (required by /work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/google/protobuf/pyext/_message.cpython-39-x86_64-linux-gnu.so)"
     ]
    }
   ],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72f916bc-dc27-4afc-8930-e3d27741986a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/boto/botocore\n",
      "  Cloning https://github.com/boto/botocore to /tmp/pip-req-build-e8pkl7zs\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/boto/botocore /tmp/pip-req-build-e8pkl7zs\n",
      "  Resolved https://github.com/boto/botocore to commit b0779ef4d040b723acb1e46c03d3c934abeaf1de\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages (from botocore==1.34.131) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages (from botocore==1.34.131) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages (from botocore==1.34.131) (1.26.19)\n",
      "Requirement already satisfied: six>=1.5 in /work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.34.131) (1.16.0)\n",
      "Building wheels for collected packages: botocore\n",
      "  Building wheel for botocore (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for botocore: filename=botocore-1.34.131-py3-none-any.whl size=12298693 sha256=427b1cbaba7e043d72e344ae0a6f6c52d0e0c577044a72cd310a629c7af81035\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-8gvjk8w_/wheels/4a/7a/78/0d35fd088fda0fc06fb0b75219fe40db96f799c8a364188df5\n",
      "Successfully built botocore\n",
      "Installing collected packages: botocore\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.31.64\n",
      "    Uninstalling botocore-1.31.64:\n",
      "      Successfully uninstalled botocore-1.31.64\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 2.7.0 requires botocore<1.31.65,>=1.31.16, but you have botocore 1.34.131 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed botocore-1.34.131\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/boto/botocore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "513db19a-88d4-455d-8bc0-bbba590fb864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightning@ git+https://github.com/Lightning-AI/lightning@master (from -r requirements.txt (line 4))\n",
      "  Cloning https://github.com/Lightning-AI/lightning (to revision master) to /tmp/pip-install-fve0zrp0/lightning_c56c0080b7c1409e8cb9693183be9b0c\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/Lightning-AI/lightning /tmp/pip-install-fve0zrp0/lightning_c56c0080b7c1409e8cb9693183be9b0c\n",
      "  Resolved https://github.com/Lightning-AI/lightning to commit 9304a2c72eeb4b9ffa80390f5428ae33afd5b85f\n",
      "  Running command git submodule update --init --recursive -q\n",
      "  Encountered 22 file(s) that should have been pointers, but weren't:\n",
      "        .notebooks/course_UvA-DL/01-introduction-to-pytorch.ipynb\n",
      "        .notebooks/course_UvA-DL/02-activation-functions.ipynb\n",
      "        .notebooks/course_UvA-DL/03-initialization-and-optimization.ipynb\n",
      "        .notebooks/course_UvA-DL/04-inception-resnet-densenet.ipynb\n",
      "        .notebooks/course_UvA-DL/05-transformers-and-MH-attention.ipynb\n",
      "        .notebooks/course_UvA-DL/06-graph-neural-networks.ipynb\n",
      "        .notebooks/course_UvA-DL/07-deep-energy-based-generative-models.ipynb\n",
      "        .notebooks/course_UvA-DL/08-deep-autoencoders.ipynb\n",
      "        .notebooks/course_UvA-DL/09-normalizing-flows.ipynb\n",
      "        .notebooks/course_UvA-DL/10-autoregressive-image-modeling.ipynb\n",
      "        .notebooks/course_UvA-DL/11-vision-transformer.ipynb\n",
      "        .notebooks/flash_tutorials/electricity_forecasting.ipynb\n",
      "        .notebooks/flash_tutorials/image_classification.ipynb\n",
      "        .notebooks/flash_tutorials/tabular_classification.ipynb\n",
      "        .notebooks/flash_tutorials/text_classification.ipynb\n",
      "        .notebooks/lightning_examples/cifar10-baseline.ipynb\n",
      "        .notebooks/lightning_examples/datamodules.ipynb\n",
      "        .notebooks/lightning_examples/finetuning-scheduler.ipynb\n",
      "        .notebooks/lightning_examples/warp-drive.ipynb\n",
      "        .notebooks/templates/img-classify.ipynb\n",
      "        .notebooks/templates/simple.ipynb\n",
      "        .notebooks/templates/titanic.ipynb\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch>=2.0.0 in /work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (2.0.0)\n",
      "Collecting sentencepiece (from -r requirements.txt (line 5))\n",
      "  Downloading sentencepiece-0.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: tqdm in /work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages (from -r requirements.txt (line 6)) (4.65.0)\n",
      "Requirement already satisfied: numpy in /work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages (from -r requirements.txt (line 7)) (1.24.3)\n",
      "Collecting bitsandbytes (from -r requirements.txt (line 9))\n",
      "  Using cached bitsandbytes-0.42.0-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting datasets (from -r requirements.txt (line 10))\n",
      "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting zstandard (from -r requirements.txt (line 11))\n",
      "  Downloading zstandard-0.22.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: more-itertools in /work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages (from -r requirements.txt (line 18)) (10.1.0)\n",
      "Collecting tiktoken==0.3.3 (from -r requirements.txt (line 19))\n",
      "  Downloading tiktoken-0.3.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting jsonargparse[signatures] (from -r requirements.txt (line 8))\n",
      "  Downloading jsonargparse-4.30.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages (from tiktoken==0.3.3->-r requirements.txt (line 19)) (2023.10.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages (from tiktoken==0.3.3->-r requirements.txt (line 19)) (2.28.2)\n",
      "Requirement already satisfied: filelock in /work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages (from torch>=2.0.0->-r requirements.txt (line 3)) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages (from torch>=2.0.0->-r requirements.txt (line 3)) (4.9.0)\n",
      "Requirement already satisfied: sympy in /work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages (from torch>=2.0.0->-r requirements.txt (line 3)) (1.12)\n",
      "Requirement already satisfied: networkx in /work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages (from torch>=2.0.0->-r requirements.txt (line 3)) (3.1)\n",
      "Requirement already satisfied: jinja2 in /work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages (from torch>=2.0.0->-r requirements.txt (line 3)) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages (from torch>=2.0.0->-r requirements.txt (line 3)) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages (from torch>=2.0.0->-r requirements.txt (line 3)) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages (from torch>=2.0.0->-r requirements.txt (line 3)) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages (from torch>=2.0.0->-r requirements.txt (line 3)) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages (from torch>=2.0.0->-r requirements.txt (line 3)) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages (from torch>=2.0.0->-r requirements.txt (line 3)) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages (from torch>=2.0.0->-r requirements.txt (line 3)) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages (from torch>=2.0.0->-r requirements.txt (line 3)) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages (from torch>=2.0.0->-r requirements.txt (line 3)) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages (from torch>=2.0.0->-r requirements.txt (line 3)) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages (from torch>=2.0.0->-r requirements.txt (line 3)) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages (from torch>=2.0.0->-r requirements.txt (line 3)) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=2.0.0->-r requirements.txt (line 3)) (60.2.0)\n",
      "Requirement already satisfied: wheel in /work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=2.0.0->-r requirements.txt (line 3)) (0.41.2)\n",
      "Requirement already satisfied: cmake in /work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages (from triton==2.0.0->torch>=2.0.0->-r requirements.txt (line 3)) (3.29.0.1)\n",
      "Requirement already satisfied: lit in /work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages (from triton==2.0.0->torch>=2.0.0->-r requirements.txt (line 3)) (18.1.2)\n",
      "Requirement already satisfied: PyYAML<8.0,>=5.4 in /work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 4)) (6.0.1)\n",
      "Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 4)) (2023.10.0)\n",
      "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 4))\n",
      "  Downloading lightning_utilities-0.11.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: packaging<25.0,>=20.0 in /work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 4)) (23.2)\n",
      "Collecting torchmetrics<3.0,>=0.7.0 (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 4))\n",
      "  Downloading torchmetrics-1.4.0.post0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting pytorch-lightning (from lightning@ git+https://github.com/Lightning-AI/lightning@master->-r requirements.txt (line 4))\n",
      "  Downloading pytorch_lightning-2.3.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting docstring-parser>=0.15 (from jsonargparse[signatures]->-r requirements.txt (line 8))\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting typeshed-client>=2.1.0 (from jsonargparse[signatures]->-r requirements.txt (line 8))\n",
      "  Downloading typeshed_client-2.5.1-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: scipy in /work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages (from bitsandbytes->-r requirements.txt (line 9)) (1.10.0)\n",
      "Collecting pyarrow>=15.0.0 (from datasets->-r requirements.txt (line 10))\n",
      "  Downloading pyarrow-16.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting pyarrow-hotfix (from datasets->-r requirements.txt (line 10))\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages (from datasets->-r requirements.txt (line 10)) (0.3.7)\n",
      "Requirement already satisfied: pandas in /work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages (from datasets->-r requirements.txt (line 10)) (2.0.3)\n",
      "Collecting requests>=2.26.0 (from tiktoken==0.3.3->-r requirements.txt (line 19))\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm (from -r requirements.txt (line 6))\n",
      "  Using cached tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting xxhash (from datasets->-r requirements.txt (line 10))\n",
      "  Downloading xxhash-3.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets->-r requirements.txt (line 10))\n",
      "  Downloading multiprocess-0.70.16-py38-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: aiohttp in /work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages (from datasets->-r requirements.txt (line 10)) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages (from datasets->-r requirements.txt (line 10)) (0.22.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages (from aiohttp->datasets->-r requirements.txt (line 10)) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages (from aiohttp->datasets->-r requirements.txt (line 10)) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages (from aiohttp->datasets->-r requirements.txt (line 10)) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages (from aiohttp->datasets->-r requirements.txt (line 10)) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages (from aiohttp->datasets->-r requirements.txt (line 10)) (1.9.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages (from aiohttp->datasets->-r requirements.txt (line 10)) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages (from requests>=2.26.0->tiktoken==0.3.3->-r requirements.txt (line 19)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages (from requests>=2.26.0->tiktoken==0.3.3->-r requirements.txt (line 19)) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages (from requests>=2.26.0->tiktoken==0.3.3->-r requirements.txt (line 19)) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages (from requests>=2.26.0->tiktoken==0.3.3->-r requirements.txt (line 19)) (2023.7.22)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages (from typeshed-client>=2.1.0->jsonargparse[signatures]->-r requirements.txt (line 8)) (6.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages (from jinja2->torch>=2.0.0->-r requirements.txt (line 3)) (2.1.3)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets->-r requirements.txt (line 10))\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages (from pandas->datasets->-r requirements.txt (line 10)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages (from pandas->datasets->-r requirements.txt (line 10)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages (from pandas->datasets->-r requirements.txt (line 10)) (2023.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages (from sympy->torch>=2.0.0->-r requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages (from importlib-resources>=1.4.0->typeshed-client>=2.1.0->jsonargparse[signatures]->-r requirements.txt (line 8)) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in /work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->datasets->-r requirements.txt (line 10)) (1.16.0)\n",
      "Downloading tiktoken-0.3.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n",
      "Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "Downloading zstandard-0.22.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
      "Downloading pyarrow-16.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading torchmetrics-1.4.0.post0-py3-none-any.whl (868 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.8/868.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading typeshed_client-2.5.1-py3-none-any.whl (606 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m606.1/606.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading jsonargparse-4.30.0-py3-none-any.whl (203 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.7/203.7 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py38-none-any.whl (132 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading pytorch_lightning-2.3.0-py3-none-any.whl (812 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.2/812.2 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.6/194.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: lightning\n",
      "  Building wheel for lightning (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lightning: filename=lightning-2.3.0-py3-none-any.whl size=1995681 sha256=8fd798a43bff06be4ba45c08a62a925a8b5fc5ec6e3bb27a23464b3c6b1f2689\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-fif2f45r/wheels/f9/79/b5/6cf0fd483c31e028bcd95d4d4ff316036f9d361c87e9d07604\n",
      "Successfully built lightning\n",
      "Installing collected packages: sentencepiece, zstandard, xxhash, tqdm, requests, pyarrow-hotfix, pyarrow, lightning-utilities, jsonargparse, docstring-parser, dill, typeshed-client, tiktoken, multiprocess, bitsandbytes, datasets, torchmetrics, pytorch-lightning, lightning\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.65.0\n",
      "    Uninstalling tqdm-4.65.0:\n",
      "      Successfully uninstalled tqdm-4.65.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.28.2\n",
      "    Uninstalling requests-2.28.2:\n",
      "      Successfully uninstalled requests-2.28.2\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 14.0.2\n",
      "    Uninstalling pyarrow-14.0.2:\n",
      "      Successfully uninstalled pyarrow-14.0.2\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.7\n",
      "    Uninstalling dill-0.3.7:\n",
      "      Successfully uninstalled dill-0.3.7\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "openxlab 0.0.37 requires requests~=2.28.2, but you have requests 2.32.3 which is incompatible.\n",
      "openxlab 0.0.37 requires tqdm~=4.65.0, but you have tqdm 4.66.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed bitsandbytes-0.42.0 datasets-2.20.0 dill-0.3.8 docstring-parser-0.16 jsonargparse-4.30.0 lightning-2.3.0 lightning-utilities-0.11.2 multiprocess-0.70.16 pyarrow-16.1.0 pyarrow-hotfix-0.6 pytorch-lightning-2.3.0 requests-2.32.3 sentencepiece-0.2.0 tiktoken-0.3.3 torchmetrics-1.4.0.post0 tqdm-4.66.4 typeshed-client-2.5.1 xxhash-3.4.1 zstandard-0.22.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e562f349-9d07-4a41-a360-01d9543d0205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/work/van-speech-nlp/jindaznb/visenv'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74ef8d1b-e0b6-4e8d-b989-3be6b6f355fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded test data\n",
      "/work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python training/WL-S_train-Copy1.py --lr 1e-3 --d 1 --pretr ...\n",
      "[rank: 0] Seed set to 1337\n",
      "loaded LLaMA checkpoint\n",
      "loaded Whisper checkpoint\n",
      "loaded LAMMA model\n",
      "Number of trainable parameters: 4.8999M\n",
      "iter 0: loss 3.2259, time: 4.83s\n",
      "{'train_iter': 0, 'train_Iter_loss': 3.2259387969970703}\n",
      "iter 1: loss 3.4445, time: 4.70s\n",
      "{'train_iter': 1, 'train_Iter_loss': 3.4445486068725586}\n",
      "iter 2: loss 1.0508, time: 5.15s\n",
      "{'train_iter': 2, 'train_Iter_loss': 1.0508391857147217}\n",
      "iter 3: loss 4.7518, time: 4.51s\n",
      "{'train_iter': 3, 'train_Iter_loss': 4.7518086433410645}\n",
      "iter 4: loss 7.5779, time: 3.34s\n",
      "{'train_iter': 4, 'train_Iter_loss': 7.577939987182617}\n",
      "iter 5: loss 2.5359, time: 5.20s\n",
      "{'train_iter': 5, 'train_Iter_loss': 2.535881757736206}\n",
      "iter 6: loss 4.5695, time: 3.49s\n",
      "{'train_iter': 6, 'train_Iter_loss': 4.569498538970947}\n",
      "{'lr': 0.0009997123287671234}\n",
      "iter 7: loss 6.2267, time: 3.86s\n",
      "{'train_iter': 7, 'train_Iter_loss': 6.226677894592285}\n",
      "iter 8: loss 4.0669, time: 3.64s\n",
      "{'train_iter': 8, 'train_Iter_loss': 4.0668816566467285}\n",
      "Traceback (most recent call last):\n",
      "  File \"training/WL-S_train-Copy1.py\", line 331, in <module>\n",
      "    main()\n",
      "  File \"training/WL-S_train-Copy1.py\", line 162, in main\n",
      "    train(fabric, model, optimizer, train_data, val_data, out_dir)\n",
      "  File \"training/WL-S_train-Copy1.py\", line 188, in train\n",
      "    logits = model(input_ids, audio_features = audio_features)\n",
      "  File \"/work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages/lightning/fabric/wrappers.py\", line 141, in forward\n",
      "    output = self._forward_module(*args, **kwargs)\n",
      "  File \"/work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/lit_llama/WL_S.py\", line 287, in forward\n",
      "    x, *_ = block(x, audio_features ,rope, mask, max_seq_length)\n",
      "  File \"/work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/lit_llama/WL_S.py\", line 214, in forward\n",
      "    h, new_kv_cache, new_adapter_kv_cache, new_w_kv_caches = self.attn(\n",
      "  File \"/work/van-speech-nlp/jindaznb/visenv/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/lit_llama/WL_S.py\", line 179, in forward\n",
      "    wy = F.scaled_dot_product_attention(q, key, value, attn_mask=wmask, dropout_p=0.0, is_causal=False)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 150.00 MiB (GPU 0; 31.74 GiB total capacity; 30.08 GiB already allocated; 93.56 MiB free; 31.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "!speaker_id='F01' && python training/WL-S_train-Copy1.py --lr 1e-3 \\\n",
    "    --d 1 \\\n",
    "    --pretrained_path 'weights/alpaca.pth' \\\n",
    "    --tokenizer_path 'weights/tokenizer.model' \\\n",
    "    --data_path \"Inference/gs_inferences/baseline_data_tiny_hypo_v2/torgo_${speaker_id}\" --dataset_name \"torgo_${speaker_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72ce147a-6fb8-40b3-bb22-db115a325dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"Inference/WL-S_inference.py\", line 11, in <module>\n",
      "    from evaluate import load\n",
      "ModuleNotFoundError: No module named 'evaluate'\n"
     ]
    }
   ],
   "source": [
    "!speaker_id='F01' && dataset_name=\"torgo_${speaker_id}\" && python Inference/WL-S_inference.py \\\n",
    "    --option 'S' \\\n",
    "    --pretrained_path 'weights/alpaca.pth' \\\n",
    "    --tokenizer_path 'weights/tokenizer.model' \\\n",
    "    --data \"Inference/gs_inferences/baseline_data_tiny_hypo_v2/torgo_${speaker_id}_test.pt\" \\\n",
    "    --save_dir \"runs/Inference\" \\\n",
    "    --root \"runs/WL_S_0.001_${dataset_name}\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f83658-fc2e-4fb1-9e1a-f4684602f073",
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_id='M03' && dataset_name=\"torgo_${speaker_id}\" && python Inference/WL-S_inference.py \\\n",
    "    --option 'S' \\\n",
    "    --pretrained_path 'weights/alpaca.pth' \\\n",
    "    --tokenizer_path 'weights/tokenizer.model' \\\n",
    "    --data \"Inference/gs_inferences/baseline_data_tiny_hypo_v2/torgo_${speaker_id}_test.pt\" \\\n",
    "    --save_dir \"runs/Inference\" \\\n",
    "    --root \"runs/WL_S_0.001_${dataset_name}\" "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
