{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f120c30a-d2d1-4cb1-b432-01305d2cc1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_To generate n-best hypothesis.ipynb    torgo.csv\n",
      "1_To generate audio_features.ipynb       torgo_test_single.ipynb\n",
      "2_finetune_whisper_on_torgo-Copy1.ipynb  Untitled.ipynb\n",
      "3_prepare_for_torgo_baseline.ipynb       \u001b[0m\u001b[01;34mwhisper-tiny\u001b[0m/\n",
      "\u001b[01;34moutput\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32c61c6a-e093-4584-82a0-c1779cf03241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/data_preparation'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74158424-3b1e-40d8-b1a7-145b9562099e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 42755979\n"
     ]
    }
   ],
   "source": [
    "!sbatch tauto.bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acc6de8c-7ffb-4648-b46f-59369751ff62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "          42744107       gpu sys/dash zhang.ji  R    6:56:15      1 d1009\n",
      "          42754428       gpu sys/dash zhang.ji  R      59:26      1 d1024\n",
      "          42755966       gpu tauto.ba zhang.ji  R       0:17      1 d1013\n"
     ]
    }
   ],
   "source": [
    "!squeue -u zhang.jinda1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "705712e0-2730-48f8-b946-3a1108c98f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!scancel 42755966"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a40010f-c43b-4024-a1d9-5efc2788ae0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'learning_rate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m     training_args_dict \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(training_args_file)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Replace the default values with the values from the command line arguments\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m training_args_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mlearning_rate\u001b[49m\n\u001b[1;32m      7\u001b[0m training_args_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mper_device_train_batch_size\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m train_batch_size\n\u001b[1;32m      8\u001b[0m training_args_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mper_device_eval_batch_size\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m eval_batch_size\n",
      "\u001b[0;31mNameError\u001b[0m: name 'learning_rate' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('training_args.json') as training_args_file:\n",
    "    training_args_dict = json.load(training_args_file)\n",
    "\n",
    "# Replace the default values with the values from the command line arguments\n",
    "training_args_dict['learning_rate'] = learning_rate\n",
    "training_args_dict['per_device_train_batch_size'] = train_batch_size\n",
    "training_args_dict['per_device_eval_batch_size'] = eval_batch_size\n",
    "training_args_dict['seed'] = seed\n",
    "training_args_dict['gradient_accumulation_steps'] = gradient_accumulation_steps\n",
    "training_args_dict['optim'] = optimizer\n",
    "training_args_dict['lr_scheduler_type'] = lr_scheduler_type\n",
    "training_args_dict['num_train_epochs'] = num_epochs\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_local_path,\n",
    "    hub_model_id=repo_name,\n",
    "    **training_args_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4136f33d-5fed-4657-9fd8-ae3d9f40de21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted: /work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/data_preparation/slurm-42713952.out\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def delete_files_with_prefix(prefix):\n",
    "    # Get the current working directory\n",
    "    current_dir = os.getcwd()\n",
    "\n",
    "    # Create a search pattern for files starting with the given prefix\n",
    "    search_pattern = os.path.join(current_dir, f\"{prefix}*\")\n",
    "\n",
    "    # Find all files matching the search pattern\n",
    "    files_to_delete = glob.glob(search_pattern)\n",
    "\n",
    "    # Iterate over the files and delete them\n",
    "    for file_path in files_to_delete:\n",
    "        try:\n",
    "            os.remove(file_path)\n",
    "            print(f\"Deleted: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting {file_path}: {e}\")\n",
    "\n",
    "# Example usage\n",
    "prefix = \"slurm-4271395\"  # Replace with your actual prefix\n",
    "delete_files_with_prefix(prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccfb1eaa-f065-4353-802a-d9c130344798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('training_args.json') as training_args_file:\n",
    "    training_args_dict = json.load(training_args_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07efbb30-44d1-4658-88e1-851357f04e3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'evaluation_strategy': 'steps',\n",
       " 'per_device_train_batch_size': 4,\n",
       " 'per_device_eval_batch_size': 4,\n",
       " 'gradient_accumulation_steps': 2,\n",
       " 'eval_delay': 0,\n",
       " 'learning_rate': 0.0001,\n",
       " 'weight_decay': 0.005,\n",
       " 'adam_beta1': 0.9,\n",
       " 'adam_beta2': 0.999,\n",
       " 'adam_epsilon': 1e-08,\n",
       " 'max_grad_norm': 1.0,\n",
       " 'max_steps': -1,\n",
       " 'lr_scheduler_type': 'linear',\n",
       " 'warmup_ratio': 0.0,\n",
       " 'warmup_steps': 1000,\n",
       " 'save_strategy': 'steps',\n",
       " 'save_steps': 500,\n",
       " 'save_total_limit': 3,\n",
       " 'report_to': 'all',\n",
       " 'seed': 42,\n",
       " 'eval_steps': 1000,\n",
       " 'num_train_epochs': 20,\n",
       " 'optim': 'adamw_torch',\n",
       " 'optim_args': None,\n",
       " 'adafactor': False,\n",
       " 'group_by_length': True,\n",
       " 'length_column_name': 'length',\n",
       " 'push_to_hub': True,\n",
       " 'hub_strategy': 'every_save'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5644c86a-ced0-4449-b303-8ad46b863578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9e99efe3ae74b6f90b94d1eba63fccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/4.60k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/transformers/models/bart/configuration_bart.py:179: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e44814cb98f44868978b03138f402bd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da0a162d70014a49b0f5a2691a267faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)_tokenizer/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f914fe75db3f4adaa0f902c3fe898002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9502aef2d8c4c43a9ed3cc3406c866d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eb581dd90644834a967d15169d96ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)tokenizer/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d030c8b3d5fd4ca089170fabad878b59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)tokenizer/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "879f8379a0a243f492db1d0233d07001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nRagRetriever requires the faiss library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/facebookresearch/faiss/blob/master/INSTALL.md and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the tokenizer, retriever, and model\u001b[39;00m\n\u001b[1;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m RagTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/rag-token-nq\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m retriever \u001b[38;5;241m=\u001b[39m \u001b[43mRagRetriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfacebook/rag-token-nq\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_dummy_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m RagSequenceForGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/rag-token-nq\u001b[39m\u001b[38;5;124m\"\u001b[39m, retriever\u001b[38;5;241m=\u001b[39mretriever)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Define the input query\u001b[39;00m\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/transformers/models/rag/retrieval_rag.py:420\u001b[0m, in \u001b[0;36mRagRetriever.from_pretrained\u001b[0;34m(cls, retriever_name_or_path, indexed_dataset, **kwargs)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_pretrained\u001b[39m(\u001b[38;5;28mcls\u001b[39m, retriever_name_or_path, indexed_dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 420\u001b[0m     \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdatasets\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfaiss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m     config \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m RagConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(retriever_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    422\u001b[0m     rag_tokenizer \u001b[38;5;241m=\u001b[39m RagTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(retriever_name_or_path, config\u001b[38;5;241m=\u001b[39mconfig)\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/transformers/utils/import_utils.py:1058\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1056\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[0;32m-> 1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nRagRetriever requires the faiss library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/facebookresearch/faiss/blob/master/INSTALL.md and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RagTokenizer, RagRetriever, RagSequenceForGeneration\n",
    "\n",
    "# Load the tokenizer, retriever, and model\n",
    "tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-token-nq\")\n",
    "retriever = RagRetriever.from_pretrained(\"facebook/rag-token-nq\", use_dummy_dataset=True)\n",
    "model = RagSequenceForGeneration.from_pretrained(\"facebook/rag-token-nq\", retriever=retriever)\n",
    "\n",
    "# Define the input query\n",
    "input_query = \"What is the capital of France?\"\n",
    "\n",
    "# Tokenize the input query\n",
    "input_ids = tokenizer(input_query, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# Generate the response\n",
    "output_ids = model.generate(input_ids)\n",
    "\n",
    "# Decode the generated response\n",
    "response = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(\"Query:\", input_query)\n",
    "print(\"Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4f9b16-d57e-4919-a212-d8d22ce8de49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
