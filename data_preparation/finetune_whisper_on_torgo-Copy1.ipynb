{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a648360f-0033-43b4-a277-dbac085af854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/SlangLab-NU/torgo_inference_on_cluster/blob/main/train.py\n",
    "# https://medium.com/@shridharpawar77/a-comprehensive-guide-for-custom-data-fine-tuning-with-the-whisper-model-60e4cbce736d\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset, DatasetDict, Audio\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union\n",
    "from evaluate import load\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5b6b80-cbc7-47e7-8492-22065d5bc3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_id = \"F01\"  # Example value; replace with the actual Speaker ID as needed\n",
    "learning_rate = 0.0001\n",
    "train_batch_size = 4\n",
    "eval_batch_size = 4\n",
    "seed = 42\n",
    "gradient_accumulation_steps = 2\n",
    "optimizer = \"adamw_torch\"\n",
    "lr_scheduler_type = \"linear\"\n",
    "num_epochs = 20\n",
    "keep_all_data = False\n",
    "debug = False\n",
    "repo_suffix = \"\"\n",
    "\n",
    "print(f\"Speaker ID: {speaker_id}\")\n",
    "print(f\"Learning rate: {learning_rate}\")\n",
    "print(f\"Training batch size: {train_batch_size}\")\n",
    "print(f\"Evaluation batch size: {eval_batch_size}\")\n",
    "print(f\"Random seed: {seed}\")\n",
    "print(f\"Gradient accumulation steps: {gradient_accumulation_steps}\")\n",
    "print(f\"Optimizer type: {optimizer}\")\n",
    "print(f\"Learning rate scheduler type: {lr_scheduler_type}\")\n",
    "print(f\"Number of epochs: {num_epochs}\")\n",
    "print(f\"Keep all data: {keep_all_data}\")\n",
    "print(f\"Debug mode: {debug}\")\n",
    "print(f\"Repository suffix: {repo_suffix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dfc483-e9d9-45db-8212-3fd11ad12ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not re.match(r'^[MF]C?[0-9]{2}$', speaker_id):\n",
    "    print(\"Please provide a valid speaker ID.\")\n",
    "    sys.exit(1)\n",
    "test_speaker = speaker_id\n",
    "\n",
    "if repo_suffix and not re.match(r'^[_-]', args.repo_suffix):\n",
    "    repo_suffix = '_' + repo_suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2460943-529b-4820-a0f6-15b1cf9c980d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"evaluation_strategy\": \"steps\",\n",
    "    \"per_device_train_batch_size\": 4,\n",
    "    \"per_device_eval_batch_size\": 4,\n",
    "    \"gradient_accumulation_steps\": 2,\n",
    "    \"eval_delay\": 0,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"weight_decay\": 0.005,\n",
    "    \"adam_beta1\": 0.9,\n",
    "    \"adam_beta2\": 0.999,\n",
    "    \"adam_epsilon\": 1e-8,\n",
    "    \"max_grad_norm\": 1.0,\n",
    "    \"max_steps\": -1,\n",
    "    \"lr_scheduler_type\": \"linear\",\n",
    "    \"warmup_ratio\": 0.0,\n",
    "    \"warmup_steps\": 1000,\n",
    "    \"save_strategy\": \"steps\",\n",
    "    \"save_steps\": 500,\n",
    "    \"save_total_limit\": 3,\n",
    "    \"report_to\": \"all\",\n",
    "    \"seed\": 42,\n",
    "    \"eval_steps\": 1000,\n",
    "    \"num_train_epochs\": 20,\n",
    "    \"optim\": \"adamw_torch\",\n",
    "    \"optim_args\": None,\n",
    "    \"adafactor\": False,\n",
    "    \"group_by_length\": True,\n",
    "    \"length_column_name\": \"length\",\n",
    "    \"push_to_hub\": True,\n",
    "    \"hub_strategy\": \"every_save\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85437382-451b-48c4-bb8f-6397955d97b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the path to the CSV file\n",
    "torgo_csv_path = \"./torgo.csv\"\n",
    "\n",
    "# Check if the path exists and is a file\n",
    "if os.path.exists(torgo_csv_path) and os.path.isfile(torgo_csv_path):\n",
    "    print(\"The CSV file exists.\")\n",
    "else:\n",
    "    print(\"The CSV file does not exist.\")\n",
    "\n",
    "torgo_dataset_path = '/work/van-speech-nlp/data/torgo'\n",
    "torgo_dataset_dir_path = torgo_dataset_path + \\\n",
    "        '/' if torgo_dataset_path[-1] != '/' else torgo_dataset_path\n",
    "output_path = 'output'\n",
    "print(f'torgo_dataset_path: {torgo_dataset_path}')\n",
    "print(f'torgo_dataset_dir_path: {torgo_dataset_dir_path}')\n",
    "\n",
    "repo_name = f'torgo_tiny_finetune_{test_speaker}{repo_suffix}'\n",
    "repo_path = f'jindaxz/{repo_name}'\n",
    "\n",
    "# Path to save model / checkpoints{repo_name}'\n",
    "model_local_path = output_path + '/model/' + repo_name\n",
    "\n",
    "pretrained_model_name = \"openai/whisper-tiny\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f951648-9797-4ff1-97ed-0a493687b3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_path + '/logs'):\n",
    "    os.makedirs(output_path + '/logs')\n",
    "\n",
    "log_dir = f'{output_path}/logs/{repo_name}'\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "log_file_name = test_speaker + '_train' + '_' + \\\n",
    "    datetime.now().strftime(\"%Y%m%d_%H%M%S\") + '.log'\n",
    "log_file_path = log_dir + '/' + log_file_name\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=log_file_path,\n",
    "    filemode='a',\n",
    "    format='%(asctime)s - %(message)s',\n",
    "    datefmt='%d-%b-%y %H:%M:%S',\n",
    "    level=logging.INFO\n",
    ")\n",
    "# Log to console\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.INFO)\n",
    "logging.getLogger().addHandler(console_handler)\n",
    "\n",
    "logging.info(\"Test Speaker: \" + test_speaker)\n",
    "logging.info(\"Log File Path: \" + log_file_path + '\\n')\n",
    "if keep_all_data:\n",
    "    logging.info(\"Keep all data in training/validation/test sets\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9381e0c5-e424-49e4-ba0a-3b977c97cc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(torgo_csv_path)\n",
    "dataset_csv = load_dataset('csv', data_files=torgo_csv_path)\n",
    "\n",
    "# Check if the following columns exist in the dataset ['session', 'audio', 'text', 'speaker_id']\n",
    "expected_columns = ['session', 'audio', 'text', 'speaker_id']\n",
    "not_found_columns = []\n",
    "for column in expected_columns:\n",
    "    if column not in dataset_csv['train'].column_names:\n",
    "        not_found_columns.append(column)\n",
    "\n",
    "not_found_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc412b0-4cde-432e-82ae-d16939c85d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\n",
    "    \"Splitting the dataset into training / validation / test sets...\")\n",
    "\n",
    "# Extract the unique speakers in the dataset\n",
    "speakers = data_df['speaker_id'].unique()\n",
    "\n",
    "logging.info(\"Unique speakers found in the dataset:\")\n",
    "logging.info(str(speakers) + '\\n')\n",
    "\n",
    "if test_speaker not in speakers:\n",
    "    logging.error(\"Test Speaker not found in the dataset.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "valid_speaker = 'F03' if test_speaker != 'F03' else 'F04'\n",
    "train_speaker = [s for s in speakers if s not in [\n",
    "    test_speaker, valid_speaker]]\n",
    "\n",
    "torgo_dataset = DatasetDict()\n",
    "torgo_dataset['train'] = dataset_csv['train'].filter(\n",
    "    lambda x: x in train_speaker, input_columns=['speaker_id'])\n",
    "torgo_dataset['validation'] = dataset_csv['train'].filter(\n",
    "    lambda x: x == valid_speaker, input_columns=['speaker_id'])\n",
    "torgo_dataset['test'] = dataset_csv['train'].filter(\n",
    "    lambda x: x == test_speaker, input_columns=['speaker_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf7ad24-7ed7-4747-a180-e15ee983e38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data_count = {'train': len(torgo_dataset['train']), 'validation': len(\n",
    "    torgo_dataset['validation']), 'test': len(torgo_dataset['test'])}\n",
    "\n",
    "if not keep_all_data:\n",
    "    # Update the three dataset splits (if ['test_data'] == 1, keep in test, if ['test_data'] == 0, keep in train and validation)\n",
    "    torgo_dataset['train'] = torgo_dataset['train'].filter(\n",
    "        lambda x: x['test_data'] == 0)\n",
    "    torgo_dataset['validation'] = torgo_dataset['validation'].filter(\n",
    "        lambda x: x['test_data'] == 0)\n",
    "    torgo_dataset['test'] = torgo_dataset['test'].filter(\n",
    "        lambda x: x['test_data'] == 1)\n",
    "\n",
    "    # Drop the 'test_data' column\n",
    "    torgo_dataset['train'] = torgo_dataset['train'].remove_columns([\n",
    "                                                                   'test_data'])\n",
    "    torgo_dataset['validation'] = torgo_dataset['validation'].remove_columns([\n",
    "                                                                             'test_data'])\n",
    "    torgo_dataset['test'] = torgo_dataset['test'].remove_columns([\n",
    "                                                                 'test_data'])\n",
    "    logging.info(\n",
    "        f\"After removal of repeated prompts, the number of data in each dataset is:\")\n",
    "    logging.info(\n",
    "        f'Train:       {len(torgo_dataset[\"train\"])}/{original_data_count[\"train\"]} ({len(torgo_dataset[\"train\"]) * 100 // original_data_count[\"train\"]}%)')\n",
    "    logging.info(\n",
    "        f'Validation:  {len(torgo_dataset[\"validation\"])}/{original_data_count[\"validation\"]} ({len(torgo_dataset[\"validation\"]) * 100 // original_data_count[\"validation\"]}%)')\n",
    "    logging.info(\n",
    "        f'Test:        {len(torgo_dataset[\"test\"])}/{original_data_count[\"test\"]} ({len(torgo_dataset[\"test\"]) * 100 // original_data_count[\"test\"]}%)\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f575ac83-fe06-48a0-a097-28b75c46125e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert the sample rate of every audio files using cast_column function\n",
    "torgo_dataset = torgo_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1545a09-007a-4613-b1b3-7b506246ca39",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import feature extractor\n",
    "from transformers import WhisperFeatureExtractor\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-tiny\")\n",
    "\n",
    "## Load WhisperTokenizer\n",
    "from transformers import WhisperTokenizer\n",
    "\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-tiny\", language=\"English\", task=\"transcribe\")\n",
    "## Combine To Create A WhisperProcessor        \n",
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\", language=\"English\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081537bb-7561-4f7a-a51d-4de1f582fedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(examples):\n",
    "    # compute log-Mel input features from input audio array \n",
    "    audio = examples[\"audio\"]\n",
    "    examples[\"input_features\"] = feature_extractor(\n",
    "        audio[\"array\"], sampling_rate=16000).input_features[0]\n",
    "    examples[\"input_length\"] = len(examples[\"input_features\"])\n",
    "    del examples[\"audio\"]\n",
    "    sentences = examples[\"text\"]\n",
    "\n",
    "    # encode target text to label ids \n",
    "    examples[\"labels\"] = tokenizer(sentences).input_ids\n",
    "    del examples[\"text\"]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3615634c-bd43-41b2-9ac9-74208f3b3444",
   "metadata": {},
   "outputs": [],
   "source": [
    "torgo_dataset = torgo_dataset.map(prepare_dataset, num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b992c275-b6f4-4cc4-9d29-f09bfb78baf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "## lets initiate the data collator\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a219ff-42dd-4cbc-9962-592ce43dc0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Filter audio within a certain length\n",
    "# min_input_length_in_sec = 1.0\n",
    "# max_input_length_in_sec = 10.0\n",
    "# sampling_rate = 16000\n",
    "\n",
    "# torgo_dataset = torgo_dataset.filter(\n",
    "#     lambda x: min_input_length_in_sec *\n",
    "#     sampling_rate < x < max_input_length_in_sec * sampling_rate,\n",
    "#     input_columns=[\"input_length\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf4efb6-da09-4e70-aa1e-ba6b7b392a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torgo_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c9c27f-1b03-4e83-bae2-c736412ccad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\n",
    "    \"After filtering audio within a certain length, the number of data in each dataset is:\")\n",
    "\n",
    "if original_data_count['train'] != 0:\n",
    "    logging.info(\n",
    "        f'Train:       {len(torgo_dataset[\"train\"])}/{original_data_count[\"train\"]} ({len(torgo_dataset[\"train\"]) * 100 // original_data_count[\"train\"]}%)')\n",
    "else:\n",
    "    logging.info(f'Train:       {len(torgo_dataset[\"train\"])}/0 (0%)')\n",
    "\n",
    "if original_data_count['validation'] != 0:\n",
    "    logging.info(\n",
    "        f'Validation:  {len(torgo_dataset[\"validation\"])}/{original_data_count[\"validation\"]} ({len(torgo_dataset[\"validation\"]) * 100 // original_data_count[\"validation\"]}%)')\n",
    "else:\n",
    "    logging.info(\n",
    "        f'Validation:  {len(torgo_dataset[\"validation\"])}/0 (0%)')\n",
    "\n",
    "if original_data_count['test'] != 0:\n",
    "    logging.info(\n",
    "        f'Test:        {len(torgo_dataset[\"test\"])}/{original_data_count[\"test\"]} ({len(torgo_dataset[\"test\"]) * 100 // original_data_count[\"test\"]}%)\\n')\n",
    "else:\n",
    "    logging.info(f'Test:        {len(torgo_dataset[\"test\"])}/0 (0%)\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61793787-21ca-423f-9639-3036492b71a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torgo_dataset = torgo_dataset.remove_columns([\"input_length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745f494e-33e9-4dd1-be73-4e0f6b0673d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9900a1-8e54-44ba-baad-b770d566fa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a Pre-Trained Checkpoint\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9393948-a900-42f8-9bba-c3b19d930c73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
