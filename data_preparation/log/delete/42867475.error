/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python training/WL-S_train-Copy1.py --lr 1e-3 --d 1 --pretr ...
[rank: 0] Seed set to 1337

-------------------------------------------------------------------------------
WL-S_train-Copy1.py 331 <module>
main()

WL-S_train-Copy1.py 162 main
train(fabric, model, optimizer, train_data, val_data, out_dir)

WL-S_train-Copy1.py 188 train
logits = model(input_ids, audio_features = audio_features)

module.py 1518 _wrapped_call_impl
return self._call_impl(*args, **kwargs)

module.py 1527 _call_impl
return forward_call(*args, **kwargs)

wrappers.py 139 forward
output = self._forward_module(*args, **kwargs)

module.py 1518 _wrapped_call_impl
return self._call_impl(*args, **kwargs)

module.py 1527 _call_impl
return forward_call(*args, **kwargs)

WL_S.py 287 forward
x, *_ = block(x, audio_features ,rope, mask, max_seq_length)

module.py 1518 _wrapped_call_impl
return self._call_impl(*args, **kwargs)

module.py 1527 _call_impl
return forward_call(*args, **kwargs)

WL_S.py 214 forward
h, new_kv_cache, new_adapter_kv_cache, new_w_kv_caches = self.attn(

module.py 1518 _wrapped_call_impl
return self._call_impl(*args, **kwargs)

module.py 1527 _call_impl
return forward_call(*args, **kwargs)

WL_S.py 179 forward
wy = F.scaled_dot_product_attention(q, key, value, attn_mask=wmask, dropout_p=0.0, is_causal=False)

torch.cuda.OutOfMemoryError:
CUDA out of memory. Tried to allocate 368.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 277.56 MiB is free. Including non-PyTorch memory, this process has 31.46 GiB memory in use. Of the allocated memory 30.35 GiB is allocated by PyTorch, and 757.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
