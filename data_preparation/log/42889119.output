Speaker ID: M05
Learning rate: 0.0001
Training batch size: 4
Evaluation batch size: 4
Random seed: 42
Gradient accumulation steps: 2
Optimizer type: adamw_torch
Learning rate scheduler type: linear
Number of epochs: 20
Keep all data: False
Debug mode: False
Repository suffix: 
The CSV file exists.
torgo_dataset_path: /work/van-speech-nlp/data/torgo
torgo_dataset_dir_path: /work/van-speech-nlp/data/torgo/
pat
whisper-v2 model loaded
train has 9,502 samples
Processing train split ...
Processed train data and saved checkpoint for M05
Processed val data and saved checkpoint for M05
Processed test data and saved checkpoint for M05
start of script
files, ['iter-000001.pth', 'iter-000002.pth', 'iter-000003.pth', 'iter-000004.pth', 'iter-000005.pth', 'iter-000006.pth', 'iter-000007.pth', 'iter-000008.pth', 'iter-000009.pth', 'iter-000010.pth', 'lit-llama-adapter-finetuned.pth']
loaded Whisper checkpoint
eveything except llama model loaded
iter-000001.pth
iter-000002.pth
iter-000003.pth
iter-000004.pth
iter-000005.pth
iter-000006.pth
iter-000007.pth
iter-000008.pth
iter-000009.pth
iter-000010.pth
lit-llama-adapter-finetuned.pth
Skipping file lit-llama-adapter-finetuned.pth due to incorrect format: invalid literal for int() with base 10: 'llama'
loaded Adapter checkpoint
