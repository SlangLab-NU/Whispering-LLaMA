/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/slam-llm/transformers/src/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/slam-llm/transformers/src/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/slam-llm/transformers/src/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Map:   0%|          | 0/9493 [00:00<?, ? examples/s]Map:  11%|█         | 1000/9493 [00:00<00:01, 7676.52 examples/s]Map:  26%|██▌       | 2422/9493 [00:00<00:00, 11094.03 examples/s]Map:  40%|████      | 3841/9493 [00:00<00:00, 12416.57 examples/s]Map:  54%|█████▍    | 5115/9493 [00:00<00:00, 12536.41 examples/s]Map:  69%|██████▊   | 6514/9493 [00:00<00:00, 13045.32 examples/s]Map:  84%|████████▎ | 7931/9493 [00:00<00:00, 13419.08 examples/s]Map: 100%|██████████| 9493/9493 [00:00<00:00, 12874.61 examples/s]Map: 100%|██████████| 9493/9493 [00:00<00:00, 12298.49 examples/s]
Map:   0%|          | 0/483 [00:00<?, ? examples/s]Map: 100%|██████████| 483/483 [00:00<00:00, 10548.71 examples/s]
Map:   0%|          | 0/442 [00:00<?, ? examples/s]Map: 100%|██████████| 442/442 [00:00<00:00, 10414.07 examples/s]
Filter:   0%|          | 0/9493 [00:00<?, ? examples/s]Filter:  11%|█         | 1000/9493 [00:16<02:20, 60.62 examples/s]Filter:  21%|██        | 2000/9493 [00:22<01:15, 99.65 examples/s]Filter:  32%|███▏      | 3000/9493 [00:27<00:52, 123.56 examples/s]Filter:  42%|████▏     | 4000/9493 [00:34<00:40, 136.03 examples/s]Filter:  53%|█████▎    | 5000/9493 [00:39<00:29, 153.00 examples/s]Filter:  63%|██████▎   | 6000/9493 [00:44<00:21, 160.42 examples/s]Filter:  74%|███████▎  | 7000/9493 [00:51<00:15, 160.11 examples/s]Filter:  84%|████████▍ | 8000/9493 [00:57<00:09, 158.54 examples/s]Filter:  95%|█████████▍| 9000/9493 [01:03<00:03, 164.31 examples/s]Filter: 100%|██████████| 9493/9493 [01:05<00:00, 165.90 examples/s]Filter: 100%|██████████| 9493/9493 [01:05<00:00, 143.97 examples/s]
Filter:   0%|          | 0/483 [00:00<?, ? examples/s]Filter: 100%|██████████| 483/483 [00:02<00:00, 169.78 examples/s]Filter: 100%|██████████| 483/483 [00:02<00:00, 169.12 examples/s]
Filter:   0%|          | 0/442 [00:00<?, ? examples/s]Filter: 100%|██████████| 442/442 [00:02<00:00, 148.67 examples/s]Filter: 100%|██████████| 442/442 [00:02<00:00, 148.19 examples/s]
Filter:   0%|          | 0/9413 [00:00<?, ? examples/s]Filter:  11%|█         | 1000/9413 [00:01<00:13, 645.83 examples/s]Filter:  21%|██        | 2000/9413 [00:03<00:11, 652.69 examples/s]Filter:  32%|███▏      | 3000/9413 [00:04<00:09, 678.84 examples/s]Filter:  42%|████▏     | 4000/9413 [00:05<00:07, 690.02 examples/s]Filter:  53%|█████▎    | 5000/9413 [00:07<00:06, 709.62 examples/s]Filter:  64%|██████▎   | 6000/9413 [00:08<00:04, 712.47 examples/s]Filter:  74%|███████▍  | 7000/9413 [00:10<00:03, 712.40 examples/s]Filter:  85%|████████▍ | 8000/9413 [00:11<00:01, 725.81 examples/s]Filter:  96%|█████████▌| 9000/9413 [00:12<00:00, 746.96 examples/s]Filter: 100%|██████████| 9413/9413 [00:13<00:00, 757.58 examples/s]Filter: 100%|██████████| 9413/9413 [00:13<00:00, 717.73 examples/s]
Filter:   0%|          | 0/482 [00:00<?, ? examples/s]Filter: 100%|██████████| 482/482 [00:00<00:00, 810.37 examples/s]Filter: 100%|██████████| 482/482 [00:00<00:00, 791.17 examples/s]
Filter:   0%|          | 0/442 [00:00<?, ? examples/s]Filter: 100%|██████████| 442/442 [00:00<00:00, 669.96 examples/s]Filter: 100%|██████████| 442/442 [00:00<00:00, 661.44 examples/s]
  0%|          | 0/9376 [00:00<?, ?it/s]  0%|          | 0/9376 [00:02<?, ?it/s]
Traceback (most recent call last):
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/0_prepare_torgo_json.py", line 351, in <module>
    generate_inference_json(train_dataset, f'torgo_train_{speaker_id}_{model_name}')
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/0_prepare_torgo_json.py", line 319, in generate_inference_json
    result, _ = whisper.decode(model, mel, options)
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/whisper_openAI/whisper/decoding.py", line 843, in decode
    result = DecodingTask(model, options).run(mel)
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/whisper_openAI/whisper/decoding.py", line 740, in run
    tokens, sum_logprobs, no_speech_probs = self._main_loop(audio_features, tokens)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/whisper_openAI/whisper/decoding.py", line 689, in _main_loop
    logits = self.inference.logits(tokens, audio_features)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/whisper_openAI/whisper/decoding.py", line 162, in logits
    return self.model.decoder(tokens, audio_features, kv_cache=self.kv_cache)
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/whisper_openAI/whisper/model.py", line 211, in forward
    x = block(x, xa, mask=self.mask, kv_cache=kv_cache)
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/whisper_openAI/whisper/model.py", line 138, in forward
    x = x + self.cross_attn(self.cross_attn_ln(x), xa, kv_cache=kv_cache)[0]
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/whisper_openAI/whisper/model.py", line 90, in forward
    wv, qk = self.qkv_attention(q, k, v, mask)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/whisper_openAI/whisper/model.py", line 102, in qkv_attention
    qk = q @ k
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 734.00 MiB. GPU 
/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/slam-llm/transformers/src/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/slam-llm/transformers/src/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/slam-llm/transformers/src/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Map:   0%|          | 0/9559 [00:00<?, ? examples/s]Map:  10%|█         | 1000/9559 [00:00<00:01, 8458.25 examples/s]Map:  25%|██▌       | 2400/9559 [00:00<00:00, 11485.91 examples/s]Map:  40%|███▉      | 3806/9559 [00:00<00:00, 12613.17 examples/s]Map:  61%|██████    | 5816/9559 [00:00<00:00, 12990.92 examples/s]Map:  82%|████████▏ | 7848/9559 [00:00<00:00, 13214.98 examples/s]Map: 100%|██████████| 9559/9559 [00:00<00:00, 12820.60 examples/s]Map: 100%|██████████| 9559/9559 [00:00<00:00, 12442.71 examples/s]
Map:   0%|          | 0/360 [00:00<?, ? examples/s]Map: 100%|██████████| 360/360 [00:00<00:00, 7860.39 examples/s]
Filter:   0%|          | 0/9559 [00:00<?, ? examples/s]Filter:  10%|█         | 1000/9559 [00:05<00:50, 169.99 examples/s]Filter:  21%|██        | 2000/9559 [00:07<00:24, 305.14 examples/s]Filter:  31%|███▏      | 3000/9559 [00:08<00:16, 408.55 examples/s]Filter:  42%|████▏     | 4000/9559 [00:10<00:11, 486.99 examples/s]Filter:  52%|█████▏    | 5000/9559 [00:11<00:08, 557.12 examples/s]Filter:  63%|██████▎   | 6000/9559 [00:13<00:06, 586.27 examples/s]Filter:  73%|███████▎  | 7000/9559 [00:14<00:04, 639.48 examples/s]Filter:  84%|████████▎ | 8000/9559 [00:15<00:02, 678.26 examples/s]Filter:  94%|█████████▍| 9000/9559 [00:16<00:00, 720.67 examples/s]Filter: 100%|██████████| 9559/9559 [00:17<00:00, 746.38 examples/s]Filter: 100%|██████████| 9559/9559 [00:17<00:00, 545.68 examples/s]
Filter:   0%|          | 0/360 [00:00<?, ? examples/s]Filter: 100%|██████████| 360/360 [00:02<00:00, 125.31 examples/s]Filter: 100%|██████████| 360/360 [00:02<00:00, 124.86 examples/s]
Filter:   0%|          | 0/9511 [00:00<?, ? examples/s]Filter:  11%|█         | 1000/9511 [00:02<00:17, 482.28 examples/s]Filter:  21%|██        | 2000/9511 [00:04<00:14, 502.32 examples/s]Filter:  32%|███▏      | 3000/9511 [00:05<00:10, 623.87 examples/s]Filter:  42%|████▏     | 4000/9511 [00:06<00:07, 706.92 examples/s]Filter:  53%|█████▎    | 5000/9511 [00:07<00:06, 747.77 examples/s]Filter:  63%|██████▎   | 6000/9511 [00:08<00:04, 793.50 examples/s]Filter:  74%|███████▎  | 7000/9511 [00:09<00:03, 818.97 examples/s]Filter:  84%|████████▍ | 8000/9511 [00:10<00:01, 864.86 examples/s]Filter:  95%|█████████▍| 9000/9511 [00:11<00:00, 905.26 examples/s]Filter: 100%|██████████| 9511/9511 [00:12<00:00, 933.73 examples/s]Filter: 100%|██████████| 9511/9511 [00:12<00:00, 778.67 examples/s]
Filter:   0%|          | 0/319 [00:00<?, ? examples/s]Filter: 100%|██████████| 319/319 [00:00<00:00, 671.01 examples/s]Filter: 100%|██████████| 319/319 [00:00<00:00, 648.70 examples/s]
  0%|          | 0/9475 [00:00<?, ?it/s]  0%|          | 0/9475 [00:02<?, ?it/s]
Traceback (most recent call last):
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/0_prepare_torgo_json.py", line 351, in <module>
    generate_inference_json(train_dataset, f'torgo_train_{speaker_id}_{model_name}')
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/0_prepare_torgo_json.py", line 319, in generate_inference_json
    result, _ = whisper.decode(model, mel, options)
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/whisper_openAI/whisper/decoding.py", line 843, in decode
    result = DecodingTask(model, options).run(mel)
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/whisper_openAI/whisper/decoding.py", line 740, in run
    tokens, sum_logprobs, no_speech_probs = self._main_loop(audio_features, tokens)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/whisper_openAI/whisper/decoding.py", line 689, in _main_loop
    logits = self.inference.logits(tokens, audio_features)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/whisper_openAI/whisper/decoding.py", line 162, in logits
    return self.model.decoder(tokens, audio_features, kv_cache=self.kv_cache)
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/whisper_openAI/whisper/model.py", line 211, in forward
    x = block(x, xa, mask=self.mask, kv_cache=kv_cache)
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/whisper_openAI/whisper/model.py", line 138, in forward
    x = x + self.cross_attn(self.cross_attn_ln(x), xa, kv_cache=kv_cache)[0]
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/whisper_openAI/whisper/model.py", line 90, in forward
    wv, qk = self.qkv_attention(q, k, v, mask)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/whisper_openAI/whisper/model.py", line 102, in qkv_attention
    qk = q @ k
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 734.00 MiB. GPU 
/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/slam-llm/transformers/src/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/slam-llm/transformers/src/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/slam-llm/transformers/src/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Map:   0%|          | 0/9594 [00:00<?, ? examples/s]Map:   8%|▊         | 749/9594 [00:00<00:01, 7439.62 examples/s]Map:  18%|█▊        | 1774/9594 [00:00<00:00, 9084.79 examples/s]Map:  32%|███▏      | 3064/9594 [00:00<00:00, 10821.13 examples/s]Map:  46%|████▋     | 4452/9594 [00:00<00:00, 12023.98 examples/s]Map:  61%|██████    | 5826/9594 [00:00<00:00, 12639.64 examples/s]Map:  74%|███████▍  | 7095/9594 [00:00<00:00, 12648.23 examples/s]Map:  88%|████████▊ | 8486/9594 [00:00<00:00, 13058.32 examples/s]Map: 100%|██████████| 9594/9594 [00:00<00:00, 11963.78 examples/s]
Map:   0%|          | 0/316 [00:00<?, ? examples/s]Map: 100%|██████████| 316/316 [00:00<00:00, 8819.30 examples/s]
Filter:   0%|          | 0/9594 [00:00<?, ? examples/s]Filter:  10%|█         | 1000/9594 [00:07<01:03, 135.59 examples/s]Filter:  21%|██        | 2000/9594 [00:08<00:28, 262.03 examples/s]Filter:  31%|███▏      | 3000/9594 [00:09<00:17, 377.01 examples/s]Filter:  42%|████▏     | 4000/9594 [00:11<00:11, 477.89 examples/s]Filter:  52%|█████▏    | 5000/9594 [00:12<00:08, 557.01 examples/s]Filter:  63%|██████▎   | 6000/9594 [00:13<00:06, 591.72 examples/s]Filter:  73%|███████▎  | 7000/9594 [00:15<00:04, 615.73 examples/s]Filter:  83%|████████▎ | 8000/9594 [00:16<00:02, 643.07 examples/s]Filter:  94%|█████████▍| 9000/9594 [00:18<00:00, 681.44 examples/s]Filter: 100%|██████████| 9594/9594 [00:18<00:00, 697.26 examples/s]Filter: 100%|██████████| 9594/9594 [00:18<00:00, 507.20 examples/s]
Filter:   0%|          | 0/316 [00:00<?, ? examples/s]Filter: 100%|██████████| 316/316 [00:02<00:00, 144.83 examples/s]Filter: 100%|██████████| 316/316 [00:02<00:00, 144.30 examples/s]
Filter:   0%|          | 0/9539 [00:00<?, ? examples/s]Filter:  10%|█         | 1000/9539 [00:01<00:08, 981.47 examples/s]Filter:  21%|██        | 2000/9539 [00:02<00:07, 985.38 examples/s]Filter:  31%|███▏      | 3000/9539 [00:03<00:06, 977.08 examples/s]Filter:  42%|████▏     | 4000/9539 [00:04<00:05, 978.69 examples/s]Filter:  52%|█████▏    | 5000/9539 [00:05<00:04, 956.66 examples/s]Filter:  63%|██████▎   | 6000/9539 [00:06<00:03, 947.75 examples/s]Filter:  73%|███████▎  | 7000/9539 [00:07<00:02, 939.27 examples/s]Filter:  84%|████████▍ | 8000/9539 [00:08<00:01, 948.39 examples/s]Filter:  94%|█████████▍| 9000/9539 [00:09<00:00, 967.50 examples/s]Filter: 100%|██████████| 9539/9539 [00:09<00:00, 989.97 examples/s]Filter: 100%|██████████| 9539/9539 [00:09<00:00, 968.26 examples/s]
Filter:   0%|          | 0/279 [00:00<?, ? examples/s]Filter: 100%|██████████| 279/279 [00:00<00:00, 732.99 examples/s]Filter: 100%|██████████| 279/279 [00:00<00:00, 712.34 examples/s]
  0%|          | 0/9502 [00:00<?, ?it/s]  0%|          | 0/9502 [00:03<?, ?it/s]
Traceback (most recent call last):
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/0_prepare_torgo_json.py", line 351, in <module>
    generate_inference_json(train_dataset, f'torgo_train_{speaker_id}_{model_name}')
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/0_prepare_torgo_json.py", line 319, in generate_inference_json
    result, _ = whisper.decode(model, mel, options)
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/whisper_openAI/whisper/decoding.py", line 843, in decode
    result = DecodingTask(model, options).run(mel)
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/whisper_openAI/whisper/decoding.py", line 740, in run
    tokens, sum_logprobs, no_speech_probs = self._main_loop(audio_features, tokens)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/whisper_openAI/whisper/decoding.py", line 689, in _main_loop
    logits = self.inference.logits(tokens, audio_features)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/whisper_openAI/whisper/decoding.py", line 162, in logits
    return self.model.decoder(tokens, audio_features, kv_cache=self.kv_cache)
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/whisper_openAI/whisper/model.py", line 211, in forward
    x = block(x, xa, mask=self.mask, kv_cache=kv_cache)
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/whisper_openAI/whisper/model.py", line 138, in forward
    x = x + self.cross_attn(self.cross_attn_ln(x), xa, kv_cache=kv_cache)[0]
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/whisper_openAI/whisper/model.py", line 90, in forward
    wv, qk = self.qkv_attention(q, k, v, mask)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/whisper_openAI/whisper/model.py", line 102, in qkv_attention
    qk = q @ k
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 734.00 MiB. GPU 
