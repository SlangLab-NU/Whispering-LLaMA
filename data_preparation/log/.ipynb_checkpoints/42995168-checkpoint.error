/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/slam-llm/transformers/src/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/slam-llm/transformers/src/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/slam-llm/transformers/src/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Map:   0%|          | 0/9519 [00:00<?, ? examples/s]Map:  11%|█         | 1000/9519 [00:00<00:01, 8228.11 examples/s]Map:  25%|██▌       | 2407/9519 [00:00<00:00, 11382.88 examples/s]Map:  41%|████      | 3873/9519 [00:00<00:00, 12807.79 examples/s]Map:  55%|█████▍    | 5219/9519 [00:00<00:00, 13055.22 examples/s]Map:  70%|██████▉   | 6656/9519 [00:00<00:00, 13517.47 examples/s]Map:  92%|█████████▏| 8793/9519 [00:00<00:00, 13827.37 examples/s]Map: 100%|██████████| 9519/9519 [00:00<00:00, 12715.85 examples/s]
Map:   0%|          | 0/407 [00:00<?, ? examples/s]Map: 100%|██████████| 407/407 [00:00<00:00, 8861.74 examples/s]
Filter:   0%|          | 0/9519 [00:00<?, ? examples/s]Filter:  11%|█         | 1000/9519 [00:14<01:59, 71.26 examples/s]Filter:  21%|██        | 2000/9519 [00:19<01:07, 110.83 examples/s]Filter:  32%|███▏      | 3000/9519 [00:25<00:49, 132.57 examples/s]Filter:  42%|████▏     | 4000/9519 [00:31<00:38, 142.55 examples/s]Filter:  53%|█████▎    | 5000/9519 [00:36<00:28, 160.00 examples/s]Filter:  63%|██████▎   | 6000/9519 [00:42<00:21, 164.92 examples/s]Filter:  74%|███████▎  | 7000/9519 [00:48<00:15, 163.14 examples/s]Filter:  84%|████████▍ | 8000/9519 [00:54<00:09, 160.79 examples/s]Filter:  95%|█████████▍| 9000/9519 [01:00<00:03, 165.58 examples/s]Filter: 100%|██████████| 9519/9519 [01:03<00:00, 167.14 examples/s]Filter: 100%|██████████| 9519/9519 [01:03<00:00, 149.96 examples/s]
Filter:   0%|          | 0/483 [00:00<?, ? examples/s]Filter: 100%|██████████| 483/483 [00:02<00:00, 169.77 examples/s]Filter: 100%|██████████| 483/483 [00:02<00:00, 169.12 examples/s]
Filter:   0%|          | 0/407 [00:00<?, ? examples/s]Filter: 100%|██████████| 407/407 [00:03<00:00, 135.47 examples/s]Filter: 100%|██████████| 407/407 [00:03<00:00, 135.16 examples/s]
Filter:   0%|          | 0/9450 [00:00<?, ? examples/s]Filter:  11%|█         | 1000/9450 [00:01<00:12, 657.90 examples/s]Filter:  21%|██        | 2000/9450 [00:03<00:11, 657.74 examples/s]Filter:  32%|███▏      | 3000/9450 [00:04<00:09, 681.80 examples/s]Filter:  42%|████▏     | 4000/9450 [00:05<00:07, 695.01 examples/s]Filter:  53%|█████▎    | 5000/9450 [00:07<00:06, 719.25 examples/s]Filter:  63%|██████▎   | 6000/9450 [00:08<00:04, 718.54 examples/s]Filter:  74%|███████▍  | 7000/9450 [00:09<00:03, 718.30 examples/s]Filter:  85%|████████▍ | 8000/9450 [00:11<00:01, 729.22 examples/s]Filter:  95%|█████████▌| 9000/9450 [00:12<00:00, 749.22 examples/s]Filter: 100%|██████████| 9450/9450 [00:13<00:00, 758.45 examples/s]Filter: 100%|██████████| 9450/9450 [00:13<00:00, 722.55 examples/s]
Filter:   0%|          | 0/482 [00:00<?, ? examples/s]Filter: 100%|██████████| 482/482 [00:00<00:00, 801.37 examples/s]Filter: 100%|██████████| 482/482 [00:00<00:00, 782.04 examples/s]
Filter:   0%|          | 0/388 [00:00<?, ? examples/s]Filter: 100%|██████████| 388/388 [00:00<00:00, 708.38 examples/s]Filter: 100%|██████████| 388/388 [00:00<00:00, 690.62 examples/s]
  0%|          | 0/9413 [00:00<?, ?it/s]  0%|          | 0/9413 [00:02<?, ?it/s]
Traceback (most recent call last):
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/0_prepare_torgo_json.py", line 351, in <module>
    generate_inference_json(train_dataset, f'torgo_train_{speaker_id}_{model_name}')
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/0_prepare_torgo_json.py", line 319, in generate_inference_json
    result, _ = whisper.decode(model, mel, options)
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/whisper_openAI/whisper/decoding.py", line 843, in decode
    result = DecodingTask(model, options).run(mel)
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/whisper_openAI/whisper/decoding.py", line 740, in run
    tokens, sum_logprobs, no_speech_probs = self._main_loop(audio_features, tokens)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/whisper_openAI/whisper/decoding.py", line 689, in _main_loop
    logits = self.inference.logits(tokens, audio_features)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/whisper_openAI/whisper/decoding.py", line 162, in logits
    return self.model.decoder(tokens, audio_features, kv_cache=self.kv_cache)
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/whisper_openAI/whisper/model.py", line 211, in forward
    x = block(x, xa, mask=self.mask, kv_cache=kv_cache)
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/whisper_openAI/whisper/model.py", line 138, in forward
    x = x + self.cross_attn(self.cross_attn_ln(x), xa, kv_cache=kv_cache)[0]
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/whisper_openAI/whisper/model.py", line 90, in forward
    wv, qk = self.qkv_attention(q, k, v, mask)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/whisper_openAI/whisper/model.py", line 102, in qkv_attention
    qk = q @ k
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 734.00 MiB. GPU 
/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/slam-llm/transformers/src/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/slam-llm/transformers/src/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/slam-llm/transformers/src/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Map:   0%|          | 0/9508 [00:00<?, ? examples/s]Map:  10%|█         | 993/9508 [00:00<00:00, 9829.52 examples/s]Map:  22%|██▏       | 2100/9508 [00:00<00:00, 10553.14 examples/s]Map:  37%|███▋      | 3487/9508 [00:00<00:00, 12059.84 examples/s]Map:  51%|█████▏    | 4881/9508 [00:00<00:00, 12796.02 examples/s]Map:  72%|███████▏  | 6862/9508 [00:00<00:00, 12981.41 examples/s]Map:  93%|█████████▎| 8856/9508 [00:00<00:00, 13100.85 examples/s]Map: 100%|██████████| 9508/9508 [00:00<00:00, 12344.45 examples/s]
Map:   0%|          | 0/423 [00:00<?, ? examples/s]Map: 100%|██████████| 423/423 [00:00<00:00, 6010.52 examples/s]
Filter:   0%|          | 0/9508 [00:00<?, ? examples/s]Filter:  11%|█         | 1000/9508 [00:04<00:37, 229.32 examples/s]Filter:  21%|██        | 2000/9508 [00:05<00:18, 405.70 examples/s]Filter:  32%|███▏      | 3000/9508 [00:06<00:12, 534.67 examples/s]Filter:  42%|████▏     | 4000/9508 [00:07<00:08, 651.14 examples/s]Filter:  53%|█████▎    | 5000/9508 [00:09<00:06, 671.62 examples/s]Filter:  63%|██████▎   | 6000/9508 [00:10<00:05, 673.80 examples/s]Filter:  74%|███████▎  | 7000/9508 [00:11<00:03, 707.24 examples/s]Filter:  84%|████████▍ | 8000/9508 [00:13<00:02, 728.19 examples/s]Filter:  95%|█████████▍| 9000/9508 [00:14<00:00, 761.89 examples/s]Filter: 100%|██████████| 9508/9508 [00:14<00:00, 780.26 examples/s]Filter: 100%|██████████| 9508/9508 [00:14<00:00, 638.33 examples/s]
Filter:   0%|          | 0/423 [00:00<?, ? examples/s]Filter: 100%|██████████| 423/423 [00:03<00:00, 122.70 examples/s]Filter: 100%|██████████| 423/423 [00:03<00:00, 122.39 examples/s]
Filter:   0%|          | 0/9439 [00:00<?, ? examples/s]Filter:  11%|█         | 1000/9439 [00:01<00:16, 517.90 examples/s]Filter:  21%|██        | 2000/9439 [00:03<00:13, 556.54 examples/s]Filter:  32%|███▏      | 3000/9439 [00:04<00:09, 693.37 examples/s]Filter:  42%|████▏     | 4000/9439 [00:05<00:06, 778.30 examples/s]Filter:  53%|█████▎    | 5000/9439 [00:06<00:05, 809.54 examples/s]Filter:  64%|██████▎   | 6000/9439 [00:07<00:04, 845.02 examples/s]Filter:  74%|███████▍  | 7000/9439 [00:09<00:02, 861.15 examples/s]Filter:  85%|████████▍ | 8000/9439 [00:10<00:01, 897.95 examples/s]Filter:  95%|█████████▌| 9000/9439 [00:11<00:00, 929.96 examples/s]Filter: 100%|██████████| 9439/9439 [00:11<00:00, 949.73 examples/s]Filter: 100%|██████████| 9439/9439 [00:11<00:00, 822.83 examples/s]
Filter:   0%|          | 0/405 [00:00<?, ? examples/s]Filter: 100%|██████████| 405/405 [00:00<00:00, 654.89 examples/s]Filter: 100%|██████████| 405/405 [00:00<00:00, 642.15 examples/s]
  0%|          | 0/9402 [00:00<?, ?it/s]  0%|          | 0/9402 [00:02<?, ?it/s]
Traceback (most recent call last):
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/0_prepare_torgo_json.py", line 351, in <module>
    generate_inference_json(train_dataset, f'torgo_train_{speaker_id}_{model_name}')
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/0_prepare_torgo_json.py", line 319, in generate_inference_json
    result, _ = whisper.decode(model, mel, options)
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/whisper_openAI/whisper/decoding.py", line 843, in decode
    result = DecodingTask(model, options).run(mel)
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/whisper_openAI/whisper/decoding.py", line 740, in run
    tokens, sum_logprobs, no_speech_probs = self._main_loop(audio_features, tokens)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/whisper_openAI/whisper/decoding.py", line 689, in _main_loop
    logits = self.inference.logits(tokens, audio_features)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/whisper_openAI/whisper/decoding.py", line 162, in logits
    return self.model.decoder(tokens, audio_features, kv_cache=self.kv_cache)
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/whisper_openAI/whisper/model.py", line 211, in forward
    x = block(x, xa, mask=self.mask, kv_cache=kv_cache)
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/whisper_openAI/whisper/model.py", line 138, in forward
    x = x + self.cross_attn(self.cross_attn_ln(x), xa, kv_cache=kv_cache)[0]
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/whisper_openAI/whisper/model.py", line 90, in forward
    wv, qk = self.qkv_attention(q, k, v, mask)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/whisper_openAI/whisper/model.py", line 102, in qkv_attention
    qk = q @ k
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 734.00 MiB. GPU 
/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/slam-llm/transformers/src/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/slam-llm/transformers/src/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/slam-llm/transformers/src/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Map:   0%|          | 0/9749 [00:00<?, ? examples/s]Map:  12%|█▏        | 1177/9749 [00:00<00:00, 11681.58 examples/s]Map:  27%|██▋       | 2609/9749 [00:00<00:00, 13223.71 examples/s]Map:  41%|████      | 4000/9749 [00:00<00:00, 13229.40 examples/s]Map:  56%|█████▌    | 5454/9749 [00:00<00:00, 13737.01 examples/s]Map:  71%|███████   | 6897/9749 [00:00<00:00, 13981.16 examples/s]Map:  92%|█████████▏| 8985/9749 [00:00<00:00, 13952.39 examples/s]Map: 100%|██████████| 9749/9749 [00:00<00:00, 13322.03 examples/s]
Map:   0%|          | 0/126 [00:00<?, ? examples/s]Map: 100%|██████████| 126/126 [00:00<00:00, 5983.59 examples/s]
Filter:   0%|          | 0/9749 [00:00<?, ? examples/s]Filter:  10%|█         | 1000/9749 [00:05<00:45, 191.52 examples/s]Filter:  21%|██        | 2000/9749 [00:06<00:20, 372.47 examples/s]Filter:  31%|███       | 3000/9749 [00:07<00:13, 496.28 examples/s]Filter:  41%|████      | 4000/9749 [00:08<00:09, 583.50 examples/s]Filter:  51%|█████▏    | 5000/9749 [00:09<00:07, 642.59 examples/s]Filter:  62%|██████▏   | 6000/9749 [00:11<00:05, 702.76 examples/s]Filter:  72%|███████▏  | 7000/9749 [00:12<00:03, 702.10 examples/s]Filter:  82%|████████▏ | 8000/9749 [00:13<00:02, 700.84 examples/s]Filter:  92%|█████████▏| 9000/9749 [00:15<00:01, 722.11 examples/s]Filter: 100%|██████████| 9749/9749 [00:16<00:00, 733.40 examples/s]Filter: 100%|██████████| 9749/9749 [00:16<00:00, 602.41 examples/s]
Filter:   0%|          | 0/126 [00:00<?, ? examples/s]Filter: 100%|██████████| 126/126 [00:00<00:00, 156.28 examples/s]Filter: 100%|██████████| 126/126 [00:00<00:00, 154.83 examples/s]
Filter:   0%|          | 0/9669 [00:00<?, ? examples/s]Filter:  10%|█         | 1000/9669 [00:01<00:10, 858.35 examples/s]Filter:  21%|██        | 2000/9669 [00:02<00:08, 945.44 examples/s]Filter:  31%|███       | 3000/9669 [00:03<00:06, 954.57 examples/s]Filter:  41%|████▏     | 4000/9669 [00:04<00:05, 966.42 examples/s]Filter:  52%|█████▏    | 5000/9669 [00:05<00:04, 958.62 examples/s]Filter:  62%|██████▏   | 6000/9669 [00:06<00:03, 951.87 examples/s]Filter:  72%|███████▏  | 7000/9669 [00:07<00:02, 953.84 examples/s]Filter:  83%|████████▎ | 8000/9669 [00:08<00:01, 962.22 examples/s]Filter:  93%|█████████▎| 9000/9669 [00:09<00:00, 992.59 examples/s]Filter: 100%|██████████| 9669/9669 [00:09<00:00, 996.30 examples/s]Filter: 100%|██████████| 9669/9669 [00:10<00:00, 965.72 examples/s]
Filter:   0%|          | 0/122 [00:00<?, ? examples/s]Filter: 100%|██████████| 122/122 [00:00<00:00, 684.04 examples/s]Filter: 100%|██████████| 122/122 [00:00<00:00, 660.64 examples/s]
  0%|          | 0/9638 [00:00<?, ?it/s]  0%|          | 0/9638 [00:02<?, ?it/s]
Traceback (most recent call last):
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/0_prepare_torgo_json.py", line 351, in <module>
    generate_inference_json(train_dataset, f'torgo_train_{speaker_id}_{model_name}')
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/0_prepare_torgo_json.py", line 319, in generate_inference_json
    result, _ = whisper.decode(model, mel, options)
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/whisper_openAI/whisper/decoding.py", line 843, in decode
    result = DecodingTask(model, options).run(mel)
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/whisper_openAI/whisper/decoding.py", line 740, in run
    tokens, sum_logprobs, no_speech_probs = self._main_loop(audio_features, tokens)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/whisper_openAI/whisper/decoding.py", line 689, in _main_loop
    logits = self.inference.logits(tokens, audio_features)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/whisper_openAI/whisper/decoding.py", line 162, in logits
    return self.model.decoder(tokens, audio_features, kv_cache=self.kv_cache)
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/whisper_openAI/whisper/model.py", line 211, in forward
    x = block(x, xa, mask=self.mask, kv_cache=kv_cache)
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/whisper_openAI/whisper/model.py", line 138, in forward
    x = x + self.cross_attn(self.cross_attn_ln(x), xa, kv_cache=kv_cache)[0]
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/whisper_openAI/whisper/model.py", line 90, in forward
    wv, qk = self.qkv_attention(q, k, v, mask)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/whisper_openAI/whisper/model.py", line 102, in qkv_attention
    qk = q @ k
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 734.00 MiB. GPU 
