/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)
  return torch._C._cuda_getDeviceCount() > 0
Filter:   0%|          | 0/16394 [00:00<?, ? examples/s]Filter:  85%|████████▌ | 14000/16394 [00:00<00:00, 104470.50 examples/s]Filter: 100%|██████████| 16394/16394 [00:00<00:00, 93000.26 examples/s] 
Filter:   0%|          | 0/16394 [00:00<?, ? examples/s]Filter: 100%|██████████| 16394/16394 [00:00<00:00, 154375.38 examples/s]
Filter:   0%|          | 0/14519 [00:00<?, ? examples/s]Filter:  21%|██        | 3000/14519 [00:00<00:00, 23476.28 examples/s]Filter:  62%|██████▏   | 9000/14519 [00:00<00:00, 38424.42 examples/s]Filter: 100%|██████████| 14519/14519 [00:00<00:00, 44066.34 examples/s]Filter: 100%|██████████| 14519/14519 [00:00<00:00, 38432.74 examples/s]
Filter:   0%|          | 0/800 [00:00<?, ? examples/s]Filter: 100%|██████████| 800/800 [00:00<00:00, 10750.11 examples/s]
Map:   0%|          | 0/9493 [00:00<?, ? examples/s]Map:   4%|▍         | 412/9493 [00:00<00:02, 4059.46 examples/s]Map:  11%|█         | 1024/9493 [00:00<00:01, 5258.54 examples/s]Map:  20%|█▉        | 1857/9493 [00:00<00:01, 6651.31 examples/s]Map:  27%|██▋       | 2585/9493 [00:00<00:01, 6896.67 examples/s]Map:  36%|███▌      | 3403/9493 [00:00<00:00, 7059.39 examples/s]Map:  43%|████▎     | 4124/9493 [00:00<00:00, 7105.04 examples/s]Map:  52%|█████▏    | 4957/9493 [00:00<00:00, 7494.53 examples/s]Map:  63%|██████▎   | 6000/9493 [00:00<00:00, 7238.39 examples/s]Map:  75%|███████▌  | 7141/9493 [00:01<00:00, 5307.63 examples/s]Map:  86%|████████▌ | 8161/9493 [00:01<00:00, 6289.43 examples/s]Map:  97%|█████████▋| 9166/9493 [00:01<00:00, 7127.38 examples/s]Map: 100%|██████████| 9493/9493 [00:01<00:00, 6507.54 examples/s]
Map:   0%|          | 0/442 [00:00<?, ? examples/s]Map: 100%|██████████| 442/442 [00:00<00:00, 4282.37 examples/s]
Filter:   0%|          | 0/9493 [00:00<?, ? examples/s]Filter:  11%|█         | 1000/9493 [00:57<08:08, 17.38 examples/s]Filter:  11%|█         | 1000/9493 [01:07<08:08, 17.38 examples/s]Filter:  21%|██        | 2000/9493 [01:08<03:44, 33.32 examples/s]Filter:  32%|███▏      | 3000/9493 [01:17<02:14, 48.42 examples/s]Filter:  42%|████▏     | 4000/9493 [01:28<01:31, 59.73 examples/s]Filter:  53%|█████▎    | 5000/9493 [01:40<01:07, 67.02 examples/s]Filter:  63%|██████▎   | 6000/9493 [01:50<00:46, 75.09 examples/s]Filter:  74%|███████▎  | 7000/9493 [02:01<00:30, 80.43 examples/s]Filter:  84%|████████▍ | 8000/9493 [02:12<00:17, 83.48 examples/s]Filter:  95%|█████████▍| 9000/9493 [02:21<00:05, 88.38 examples/s]Filter: 100%|██████████| 9493/9493 [02:26<00:00, 91.73 examples/s]Filter: 100%|██████████| 9493/9493 [02:26<00:00, 64.84 examples/s]
Filter:   0%|          | 0/442 [00:00<?, ? examples/s]Filter: 100%|██████████| 442/442 [00:04<00:00, 99.57 examples/s]Filter: 100%|██████████| 442/442 [00:04<00:00, 99.06 examples/s]
Filter:   0%|          | 0/9413 [00:00<?, ? examples/s]Filter:  11%|█         | 1000/9413 [00:03<00:32, 258.95 examples/s]Filter:  21%|██        | 2000/9413 [00:07<00:26, 275.40 examples/s]Filter:  32%|███▏      | 3000/9413 [00:10<00:22, 278.89 examples/s]Filter:  42%|████▏     | 4000/9413 [00:14<00:19, 271.26 examples/s]Filter:  53%|█████▎    | 5000/9413 [00:18<00:16, 268.79 examples/s]Filter:  64%|██████▎   | 6000/9413 [00:22<00:12, 272.99 examples/s]Filter:  74%|███████▍  | 7000/9413 [00:25<00:08, 273.29 examples/s]Filter:  85%|████████▍ | 8000/9413 [00:29<00:05, 278.96 examples/s]Filter:  96%|█████████▌| 9000/9413 [00:32<00:01, 289.72 examples/s]Filter: 100%|██████████| 9413/9413 [00:33<00:00, 293.38 examples/s]Filter: 100%|██████████| 9413/9413 [00:33<00:00, 280.06 examples/s]
Filter:   0%|          | 0/442 [00:00<?, ? examples/s]Filter: 100%|██████████| 442/442 [00:01<00:00, 305.14 examples/s]Filter: 100%|██████████| 442/442 [00:01<00:00, 302.41 examples/s]
  0%|          | 0/9376 [00:00<?, ?it/s]  0%|          | 0/9376 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/3_prepare_for_torgo_baseline.py", line 369, in <module>
    generate_inference_json(train_dataset, f'torgo_train_{speaker_id}')
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/3_prepare_for_torgo_baseline.py", line 338, in generate_inference_json
    result, _ = whisper.decode(model, mel, options)
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/whisper_openAI/whisper/decoding.py", line 843, in decode
    result = DecodingTask(model, options).run(mel)
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/whisper_openAI/whisper/decoding.py", line 720, in run
    audio_features: Tensor = self._get_audio_features(mel)  # encoder forward pass
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/whisper_openAI/whisper/decoding.py", line 656, in _get_audio_features
    audio_features = self.model.encoder(mel)
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/whisper_openAI/whisper/model.py", line 162, in forward
    x = F.gelu(self.conv1(x))
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/nn/modules/conv.py", line 310, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/Whispering-LLaMA/whisper_openAI/whisper/model.py", line 48, in _conv_forward
    return super()._conv_forward(
  File "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torch/nn/modules/conv.py", line 306, in _conv_forward
    return F.conv1d(input, weight, bias, self.stride,
RuntimeError: "slow_conv2d_cpu" not implemented for 'Half'
