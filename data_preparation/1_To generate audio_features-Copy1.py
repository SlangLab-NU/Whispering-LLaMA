#!/usr/bin/env python
# coding: utf-8

# In[2]:


# !pip show openai-whisper
get_ipython().run_line_magic('cd', '..')
get_ipython().system('ls')


# In[2]:


import os
from dataclasses import dataclass, field, replace
from typing import TYPE_CHECKING, Dict, Iterable, List, Optional, Sequence, Tuple, Union
import whisper_openAI.whisper as whisper
import torch
from whisper_openAI.whisper.tokenizer import Tokenizer, get_tokenizer
import torch
import torch.nn.functional as F
from torch import Tensor

# We get the acoustic embeddings from Whisper Large V2
model,processor = whisper.load_model("large-v2")
# model,processor = whisper.load_model("medium")


# In[3]:


model.eval()


# In[1]:


import json
k=17
# The below is the json file you can generate using the "To generatn-best hyporhesis.ipynb" notebook; Need to further tokenize the hypothesis

with open(f'Inference/gs_inferences/{k}Science and Technology.json', "r") as file: #change
    test_data = json.load(file)


# In[6]:


test_data


# In[7]:


# !pip install safetensors


# In[8]:


"""Implementation derived from https://github.com/tloen/alpaca-lora"""
import sys
from pathlib import Path
import torch
import requests
import json
import os 

from lit_llama.tokenizer import Tokenizer
from tqdm import tqdm


# In[9]:


tokenizer_path: Path = Path("weights/tokenizer.model")
tokenizer = Tokenizer(tokenizer_path)
train_set =test_data
print(f"train has {len(train_set):,} samples")
print("Processing train split ...")


# In[10]:


model.device


# In[11]:


result = []

def tokenize(tokenizer: Tokenizer, string: str, max_length: int, eos=True) -> torch.Tensor:
    return tokenizer.encode(string, bos=True, eos=eos, max_length=max_length)

instruction = 'You are an ASR transcript selector. You have a few transcripts generated by an automatic speech recognition model. Your task is to generate the most likely transcript from them. If the generated transcripts have grammatical or logical errors, you will modify them accordingly to produce the most accurate and coherent transcript.'

for i in tqdm(range(len(test_data))):        
    for name in test_data[i].keys():
        ip = test_data[i][name]
    inference = ip['inference']
    gt = ip['ground_truth']
        
    # Removing the ground_truth, if present among the inferences for the prompt
    if gt in inference:
        inference.remove(gt)
            
        # joining the inputs with '\n'
    for_input = '\n'.join(inference[:15])
    # The prompt follows the Aplaca template ; https://github.com/tatsu-lab/stanford_alpaca
    full_prompt = f"""Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n ### Instruction:\n{instruction}\n\n### Input:\n{for_input}\n\n### Response:"""
    full_prompt_and_response = full_prompt + gt

    encoded_full_prompt = tokenize(tokenizer, full_prompt, max_length=2048, eos=False)
    encoded_full_prompt_and_response = tokenize(tokenizer, full_prompt_and_response, eos=True, max_length=2048)
    labels = encoded_full_prompt_and_response.clone()
    labels_with_masked_input = encoded_full_prompt_and_response.clone()
    labels_with_masked_input[:len(encoded_full_prompt)] = -1
    
    path =  ip['path']
    audio = whisper.load_audio(path)  
    audio = whisper.pad_or_trim(audio)            
    mel = whisper.log_mel_spectrogram(audio).to(model.device)  # of shape (#channels =  80 ; #samples = )   
    mel = mel.unsqueeze(0)
    
    with torch.no_grad():
        audio_features  = model.encoder(mel)
        

    result.append( {**ip,'index':name, "input_ids": encoded_full_prompt_and_response, "input_ids_no_response": encoded_full_prompt, "labels": labels, 'labels_with_masked_input': labels_with_masked_input,'audio_features': audio_features.bfloat16()})


# In[18]:


import pickle

# Define the file path
output_file = f'Inference/gs_inferences/{k}_result.pkl'

# Save the result variable to a file
with open(output_file, 'wb') as f:
    pickle.dump(result, f)


# In[ ]:


print(len(result))
result[0]


# In[13]:


torch.save(result,'Inference/gs_inferences/gigaspeech_train.pt')


# In[1]:


print()


# In[4]:


import torch
train_data = torch.load('Inference/gs_inferences/gigaspeech_train.pt',map_location=torch.device('cpu'))


# In[5]:


len(train_data)


# In[7]:


import random

k=17
total_samples = len(train_data)
test_size = 1000

# Randomly select 1000 samples for the test set
test_indices = random.sample(range(total_samples), test_size)
train_indices = [i for i in range(total_samples) if i not in test_indices]

# Split the dataset into train and test sets
train_set = [train_data[i] for i in train_indices]
test_set = [train_data[i] for i in test_indices]


# In[8]:


len(train_set)


# In[9]:


len(test_set)


# In[10]:


# Save train and test sets into separate .pt files
torch.save(train_set, f'Inference/gs_inferences/{k}_train.pt')
torch.save(test_set, f'Inference/gs_inferences/{k}_test.pt')


# In[13]:


train_set[0]


# In[ ]:




