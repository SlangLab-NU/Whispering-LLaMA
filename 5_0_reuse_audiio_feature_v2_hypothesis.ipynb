{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc47e98b-26b5-486b-80d9-891d2de465f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import requests\n",
    "import json\n",
    "import os \n",
    "\n",
    "model_name=\"large-v2\"\n",
    "speaker_id=\"M02\"\n",
    "test_speaker=speaker_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a28494ff-4b23-4ae4-8206-45bb16a0fd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field, replace\n",
    "from typing import TYPE_CHECKING, Dict, Iterable, List, Optional, Sequence, Tuple, Union\n",
    "import whisper_openAI.whisper as whisper\n",
    "import torch\n",
    "from whisper_openAI.whisper.tokenizer import Tokenizer, get_tokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "# We get the acoustic embeddings from Whisper Large V2\n",
    "model,processor = whisper.load_model(\"large-v2\")\n",
    "# model,processor = whisper.load_model(\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b559658-5251-407c-a1ba-80aa643ba849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Whisper(\n",
       "  (encoder): AudioEncoder(\n",
       "    (conv1): Conv1d(80, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (blocks): ModuleList(\n",
       "      (0-31): 32 x ResidualAttentionBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): TextDecoder(\n",
       "    (token_embedding): Embedding(51865, 1280)\n",
       "    (blocks): ModuleList(\n",
       "      (0-31): 32 x ResidualAttentionBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (cross_attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (cross_attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19a8d9fd-6497-4bcb-913c-215652d96294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# The below is the json file you can generate using the \"To generatn-best hyporhesis.ipynb\" notebook; Need to further tokenize the hypothesis\n",
    "\n",
    "with open(f'Inference/gs_inferences/torgo_train_{speaker_id}_{model_name}.json', \"r\") as file:  # Change the file path and name here\n",
    "    train_data = json.load(file)\n",
    "\n",
    "with open(f'Inference/gs_inferences/torgo_val_{speaker_id}_{model_name}.json', \"r\") as valid_file:\n",
    "    val_data = json.load(valid_file)\n",
    "\n",
    "# Load the test set\n",
    "with open(f'Inference/gs_inferences/torgo_test_{speaker_id}_{model_name}.json', \"r\") as test_file:\n",
    "    test_data = json.load(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22849570-b168-4fa5-90e9-722b7c8d24f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'F01-Session1-arrayMic-0034': {'temp': 0.9299999999999999,\n",
       "  'path': '/work/van-speech-nlp/data/torgo/F01/Session1/wav_arrayMic/0034.wav',\n",
       "  'ground_truth': 'feet',\n",
       "  'inference': ['be',\n",
       "   'dean',\n",
       "   'like that girl from tenai yeah please',\n",
       "   'ping',\n",
       "   'please',\n",
       "   'please believe me',\n",
       "   'raf',\n",
       "   'see',\n",
       "   'well wipe you clean'],\n",
       "  'source': 'NP-Torgo',\n",
       "  'category': 'NP-Torgo',\n",
       "  'time': 30.0}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c0fff03-0459-4b99-82fb-bffc5a97f45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/work/van-speech-nlp/jindaznb/jslpnb/mllm_expriments/slam-llm/transformers/src/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train has 9,402 samples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from lit_llama.tokenizer import Tokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "tokenizer_path: Path = Path(\"weights/tokenizer.model\")\n",
    "tokenizer = Tokenizer(tokenizer_path)\n",
    "print(f\"train has {len(train_data):,} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a662385e-1aa9-4203-aa31-6c72f91e753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "old_data_train = torch.load(f'Inference/gs_inferences/baseline_data_tiny_hypo_v2/torgo_{test_speaker}_train.pt',map_location=torch.device('cpu'))\n",
    "old_data_val = torch.load(f'Inference/gs_inferences/baseline_data_tiny_hypo_v2/torgo_{test_speaker}_val.pt',map_location=torch.device('cpu'))\n",
    "old_data_test = torch.load(f'Inference/gs_inferences/baseline_data_tiny_hypo_v2/torgo_{test_speaker}_test.pt',map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab69f6bd-0174-4dcd-bc26-554e79e091b7",
   "metadata": {},
   "source": [
    "# check if data index are same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70bcec40-bafb-4185-8e1a-52b66446d250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'temp': 0.77,\n",
       " 'path': '/work/van-speech-nlp/data/torgo/F01/Session1/wav_arrayMic/0034.wav',\n",
       " 'ground_truth': 'feet',\n",
       " 'inference': ['ah ah',\n",
       "  'b',\n",
       "  'beee',\n",
       "  'beep',\n",
       "  'beep!',\n",
       "  'boom',\n",
       "  'bye',\n",
       "  'c',\n",
       "  'cf',\n",
       "  'cuuu',\n",
       "  'everything',\n",
       "  'f',\n",
       "  'fear',\n",
       "  'fee',\n",
       "  'fee!',\n",
       "  'fee!!',\n",
       "  'feeeee',\n",
       "  'feeeeeee!',\n",
       "  'feeeeeeee',\n",
       "  'feeeing',\n",
       "  'feez!',\n",
       "  'fehm',\n",
       "  'female',\n",
       "  'ferien',\n",
       "  'fie',\n",
       "  'fii',\n",
       "  'fii!',\n",
       "  'fiii',\n",
       "  'fiiiie',\n",
       "  'fiiit!',\n",
       "  'fiim',\n",
       "  'fjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjj',\n",
       "  'hehe!',\n",
       "  'here',\n",
       "  'hey',\n",
       "  'hey!',\n",
       "  'hmm',\n",
       "  'i must know that its just like a clown',\n",
       "  'i think',\n",
       "  'it p l',\n",
       "  'its',\n",
       "  'its been a while',\n",
       "  'lil lil lil lil lil lil lil lil lil lil lil lil lil',\n",
       "  'lucky!',\n",
       "  'me',\n",
       "  'ok!',\n",
       "  'okay…',\n",
       "  'p',\n",
       "  'pee',\n",
       "  'peeeeee',\n",
       "  'peein!!',\n",
       "  'phi',\n",
       "  'please',\n",
       "  'ready three',\n",
       "  'see',\n",
       "  'see ya',\n",
       "  'see ya!',\n",
       "  'see you',\n",
       "  'see you!',\n",
       "  'see!',\n",
       "  'thaaay',\n",
       "  'thank you',\n",
       "  'thank you!',\n",
       "  'the',\n",
       "  'the end!',\n",
       "  'the sceeeeee',\n",
       "  'theatheé',\n",
       "  'theres something here',\n",
       "  'thing',\n",
       "  'three',\n",
       "  'three!',\n",
       "  'uhrei',\n",
       "  'umm!',\n",
       "  'what if',\n",
       "  'who',\n",
       "  'woo',\n",
       "  'yeah',\n",
       "  'yeee',\n",
       "  'you'],\n",
       " 'source': 'NP-Torgo',\n",
       " 'category': 'NP-Torgo',\n",
       " 'time': 30.0,\n",
       " 'index': 'F01-Session1-arrayMic-0034',\n",
       " 'input_ids': tensor([    1, 13866,   338,   385, 15278,   393, 16612,   263,  3414, 29892,\n",
       "          3300,  2859,   411,   385,  1881,   393,  8128,  4340,  3030, 29889,\n",
       "         14350,   263,  2933,   393,  7128,  2486,  1614,  2167,   278,  2009,\n",
       "         29889,    13,    13,  2277, 29937,  2799,  4080, 29901,    13,  3492,\n",
       "           526,   385,  3339, 29934,  1301,   924, 11764, 29889,   887,   505,\n",
       "           263,  2846,  1301,   924, 29879,  5759,   491,   385, 18428, 12032,\n",
       "         19679,  1904, 29889,  3575,  3414,   338,   304,  5706,   278,  1556,\n",
       "          5517,  1301,   924,   515,   963, 29889,   960,   278,  5759,  1301,\n",
       "           924, 29879,   505, 14961,  2922,   936,   470, 16667,  4436, 29892,\n",
       "           366,   674,  6623,   963, 16205,   304,  7738,   278,  1556, 16232,\n",
       "           322, 16165,   261,   296,  1301,   924, 29889,    13,    13,  2277,\n",
       "         29937, 10567, 29901,    13,   801, 21023,    13, 29890,    13,   915,\n",
       "          3905,    13,   915,  1022,    13,   915,  1022, 29991,    13,   833,\n",
       "           290,    13, 26966,    13, 29883,    13,  6854,    13,  4979, 29884,\n",
       "         29884,    13, 17991,  1918,    13, 29888,    13, 29888,   799,    13,\n",
       "          1725, 29872,    13,  1725, 29872, 29991,    13,    13,  2277, 29937,\n",
       "         13291, 29901,  1725,   300,     2], dtype=torch.int32),\n",
       " 'input_ids_no_response': tensor([    1, 13866,   338,   385, 15278,   393, 16612,   263,  3414, 29892,\n",
       "          3300,  2859,   411,   385,  1881,   393,  8128,  4340,  3030, 29889,\n",
       "         14350,   263,  2933,   393,  7128,  2486,  1614,  2167,   278,  2009,\n",
       "         29889,    13,    13,  2277, 29937,  2799,  4080, 29901,    13,  3492,\n",
       "           526,   385,  3339, 29934,  1301,   924, 11764, 29889,   887,   505,\n",
       "           263,  2846,  1301,   924, 29879,  5759,   491,   385, 18428, 12032,\n",
       "         19679,  1904, 29889,  3575,  3414,   338,   304,  5706,   278,  1556,\n",
       "          5517,  1301,   924,   515,   963, 29889,   960,   278,  5759,  1301,\n",
       "           924, 29879,   505, 14961,  2922,   936,   470, 16667,  4436, 29892,\n",
       "           366,   674,  6623,   963, 16205,   304,  7738,   278,  1556, 16232,\n",
       "           322, 16165,   261,   296,  1301,   924, 29889,    13,    13,  2277,\n",
       "         29937, 10567, 29901,    13,   801, 21023,    13, 29890,    13,   915,\n",
       "          3905,    13,   915,  1022,    13,   915,  1022, 29991,    13,   833,\n",
       "           290,    13, 26966,    13, 29883,    13,  6854,    13,  4979, 29884,\n",
       "         29884,    13, 17991,  1918,    13, 29888,    13, 29888,   799,    13,\n",
       "          1725, 29872,    13,  1725, 29872, 29991,    13,    13,  2277, 29937,\n",
       "         13291, 29901], dtype=torch.int32),\n",
       " 'labels': tensor([    1, 13866,   338,   385, 15278,   393, 16612,   263,  3414, 29892,\n",
       "          3300,  2859,   411,   385,  1881,   393,  8128,  4340,  3030, 29889,\n",
       "         14350,   263,  2933,   393,  7128,  2486,  1614,  2167,   278,  2009,\n",
       "         29889,    13,    13,  2277, 29937,  2799,  4080, 29901,    13,  3492,\n",
       "           526,   385,  3339, 29934,  1301,   924, 11764, 29889,   887,   505,\n",
       "           263,  2846,  1301,   924, 29879,  5759,   491,   385, 18428, 12032,\n",
       "         19679,  1904, 29889,  3575,  3414,   338,   304,  5706,   278,  1556,\n",
       "          5517,  1301,   924,   515,   963, 29889,   960,   278,  5759,  1301,\n",
       "           924, 29879,   505, 14961,  2922,   936,   470, 16667,  4436, 29892,\n",
       "           366,   674,  6623,   963, 16205,   304,  7738,   278,  1556, 16232,\n",
       "           322, 16165,   261,   296,  1301,   924, 29889,    13,    13,  2277,\n",
       "         29937, 10567, 29901,    13,   801, 21023,    13, 29890,    13,   915,\n",
       "          3905,    13,   915,  1022,    13,   915,  1022, 29991,    13,   833,\n",
       "           290,    13, 26966,    13, 29883,    13,  6854,    13,  4979, 29884,\n",
       "         29884,    13, 17991,  1918,    13, 29888,    13, 29888,   799,    13,\n",
       "          1725, 29872,    13,  1725, 29872, 29991,    13,    13,  2277, 29937,\n",
       "         13291, 29901,  1725,   300,     2], dtype=torch.int32),\n",
       " 'labels_with_masked_input': tensor([  -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "           -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "           -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "           -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "           -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "           -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "           -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "           -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "           -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "           -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "           -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "           -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "           -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "           -1,   -1,   -1,   -1,   -1,   -1, 1725,  300,    2],\n",
       "        dtype=torch.int32),\n",
       " 'audio_features': tensor([[[-1.1328e+00, -6.0938e-01, -1.2500e+00,  ..., -3.1562e+00,\n",
       "           -3.3984e-01,  3.1445e-01],\n",
       "          [-5.0781e-01, -3.1836e-01, -9.1016e-01,  ..., -2.9531e+00,\n",
       "           -3.8867e-01,  3.4570e-01],\n",
       "          [ 7.3047e-01,  5.7422e-01, -3.5352e-01,  ..., -9.5312e-01,\n",
       "           -6.6016e-01, -7.3047e-01],\n",
       "          ...,\n",
       "          [-7.1106e-03, -4.8218e-03, -1.2146e-02,  ...,  6.4087e-03,\n",
       "           -7.2632e-03, -4.7607e-03],\n",
       "          [-8.7280e-03, -1.7166e-03, -1.4099e-02,  ...,  6.2561e-03,\n",
       "           -6.7444e-03, -5.4016e-03],\n",
       "          [-1.0803e-02,  4.2419e-03, -1.4221e-02,  ...,  2.0752e-03,\n",
       "           -7.2479e-04, -1.0986e-02]]], dtype=torch.bfloat16)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_data_train[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42aa313e-44d0-4cbe-9810-5f6026f191df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of old_data_train: 9402\n",
      "Length of old_data_val: 460\n",
      "Length of old_data_test: 403\n"
     ]
    }
   ],
   "source": [
    "# Print the lengths of the loaded data\n",
    "print(f'Length of old_data_train: {len(old_data_train)}')\n",
    "print(f'Length of old_data_val: {len(old_data_val)}')\n",
    "print(f'Length of old_data_test: {len(old_data_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a06b294-eb10-49d1-9a3f-cfa39b1d72e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mstop\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4cd37f-6648-4612-95b1-6af9b538c443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(tokenizer: Tokenizer, string: str, max_length: int, eos=True) -> torch.Tensor:\n",
    "    return tokenizer.encode(string, bos=True, eos=eos, max_length=max_length)\n",
    "    \n",
    "def process_train_data(train_data, old_data):\n",
    "    instruction = 'You are an ASR transcript selector. You have a few transcripts generated by an automatic speech recognition model. Your task is to generate the most likely transcript from them. If the generated transcripts have grammatical or logical errors, you will modify them accordingly to produce the most accurate and coherent transcript.'\n",
    "    result = []\n",
    "\n",
    "    for i in tqdm(range(len(train_data))):        \n",
    "        for name in train_data[i].keys():\n",
    "            ip = train_data[i][name]\n",
    "        inference = ip['inference']\n",
    "        gt = ip['ground_truth']\n",
    "            \n",
    "        # Removing the ground_truth, if present among the inferences for the prompt\n",
    "        if gt in inference:\n",
    "            inference.remove(gt)\n",
    "                \n",
    "        # Joining the inputs with '\\n'\n",
    "        for_input = '\\n'.join(inference[:15])\n",
    "        # The prompt follows the Alpaca template\n",
    "        full_prompt = f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{for_input}\\n\\n### Response:\"\"\"\n",
    "        full_prompt_and_response = full_prompt + gt\n",
    "\n",
    "        encoded_full_prompt = tokenize(tokenizer, full_prompt, max_length=2048, eos=False)\n",
    "        encoded_full_prompt_and_response = tokenize(tokenizer, full_prompt_and_response, eos=True, max_length=2048)\n",
    "        labels = encoded_full_prompt_and_response.clone()\n",
    "        labels_with_masked_input = encoded_full_prompt_and_response.clone()\n",
    "        labels_with_masked_input[:len(encoded_full_prompt)] = -1\n",
    "        \n",
    "        # path = ip['path']\n",
    "        # audio = whisper.load_audio(path)  \n",
    "        # audio = whisper.pad_or_trim(audio)            \n",
    "        # mel = whisper.log_mel_spectrogram(audio).to(model.device)  # Adjust as needed for your model\n",
    "        # mel = mel.unsqueeze(0)\n",
    "        \n",
    "        # with torch.no_grad():\n",
    "        #     audio_features = model.encoder(mel)\n",
    "        audio_features = old_data[i][\"audio_features\"]\n",
    "        \n",
    "        result.append({**ip, 'index': name, \"input_ids\": encoded_full_prompt_and_response, \"input_ids_no_response\": encoded_full_prompt, \"labels\": labels, 'labels_with_masked_input': labels_with_masked_input, 'audio_features': audio_features.bfloat16()})\n",
    "    print(len(result))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c6b789-6786-4e9c-8b06-76d6c38060f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = \"train\"\n",
    "result = process_train_data(train_data, old_data_train)\n",
    "torch.save(result,f'Inference/gs_inferences/torgo_{speaker_id}_{model_name}_{split}.pt')\n",
    "print(f\"Processed {split} data and saved checkpoint for {speaker_id}\")\n",
    "\n",
    "split = \"val\"\n",
    "result = process_train_data(val_data, old_data_val)\n",
    "torch.save(result,f'Inference/gs_inferences/torgo_{speaker_id}_{model_name}_{split}.pt')\n",
    "print(f\"Processed {split} data and saved checkpoint for {speaker_id}\")\n",
    "\n",
    "split = \"test\"\n",
    "result = process_train_data(test_data, old_data_test)\n",
    "torch.save(result,f'Inference/gs_inferences/torgo_{speaker_id}_{model_name}_{split}.pt')\n",
    "print(f\"Processed {split} data and saved checkpoint for {speaker_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62de5d8a-28cc-4579-be31-5ce173e9d36c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
